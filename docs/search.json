[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary - CASA0023",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  week_2",
    "section": "",
    "text": "For my Xaringan presentation please click or copy the following link:\nhttps://jinting914113.github.io/CASA23_RS/\nNo entries for this week"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3",
    "section": "",
    "text": "4 Week 3 - Remote sensing data:"
  },
  {
    "objectID": "week_3.html#geometric-correction",
    "href": "week_3.html#geometric-correction",
    "title": "3  Week 3",
    "section": "6.1 3.1.1 Geometric Correction",
    "text": "6.1 3.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\n\n6.1.1 How:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\n\n\n6.1.2 Corrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)"
  },
  {
    "objectID": "week_3.html#atmospheric-correction",
    "href": "week_3.html#atmospheric-correction",
    "title": "3  Week 3",
    "section": "6.2 3.1.2 Atmospheric Correction",
    "text": "6.2 3.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\n\n6.2.1 Why is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\n\n\n6.2.2 Corrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance."
  },
  {
    "objectID": "week_3.html#orthorectification-topographic-correction",
    "href": "week_3.html#orthorectification-topographic-correction",
    "title": "3  Week 3",
    "section": "6.3 3.1.3 Orthorectification / Topographic Correction",
    "text": "6.3 3.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\n\n6.3.1 How:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\n\n\n6.3.2 Corrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed."
  },
  {
    "objectID": "week_3.html#radiometric-calibration",
    "href": "week_3.html#radiometric-calibration",
    "title": "3  Week 3",
    "section": "6.4 3.1.4 Radiometric Calibration",
    "text": "6.4 3.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\n\n6.4.1 How:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\n\n\n6.4.2 Corrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\n\n\n6.4.3 Summary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials."
  },
  {
    "objectID": "week_3.html#feathering",
    "href": "week_3.html#feathering",
    "title": "3  Week 3",
    "section": "7.1 3.2.1 Feathering",
    "text": "7.1 3.2.1 Feathering\n\n7.1.1 Data Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\n\n\n7.1.2 Feathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\n\n\n7.1.3 Practical\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area."
  },
  {
    "objectID": "week_3.html#image-enhancement",
    "href": "week_3.html#image-enhancement",
    "title": "3  Week 3",
    "section": "7.2 3.2.2 Image enhancement",
    "text": "7.2 3.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\n\n7.2.0.1 3.2.2.1 Contrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\n\n\n7.2.1 3.2.2.2 Ratio:\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\n\n7.2.1.1 Usage:\nFor instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\n\n\n7.2.1.2 Practical:\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\n\n7.2.2 3.2.2.3 Filtering:\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\n\n7.2.2.1 Usage:\nLow-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\n\n\n7.2.2.2 Practical:\nThe Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\n\n\n7.2.3 3.2.2.4 PCA:\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\n\n7.2.3.1 Usage:\nApplied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\n\n\n7.2.3.2 Practical:\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\n\n7.2.4 3.2.2.5 Texture:\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\n\n7.2.4.1 Usage:\nEnhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\n\n\n7.2.4.2 Practical:\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the GLCMTextures package can be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\n\n7.2.5 3.2.2.6 Fusion:\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\n\n7.2.5.1 Usage:\nIn Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\n\n\n7.2.5.2 Practical:\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data. In R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#geometric-correction-1",
    "href": "week_3.html#geometric-correction-1",
    "title": "3  Week 3",
    "section": "8.1 3.1.1 Geometric Correction",
    "text": "8.1 3.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\n\n8.1.1 How:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\n\n\n8.1.2 Corrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)"
  },
  {
    "objectID": "week_3.html#atmospheric-correction-1",
    "href": "week_3.html#atmospheric-correction-1",
    "title": "3  Week 3",
    "section": "8.2 3.1.2 Atmospheric Correction",
    "text": "8.2 3.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\n\n8.2.1 Why is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\n\n\n8.2.2 Corrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance."
  },
  {
    "objectID": "week_3.html#orthorectification-topographic-correction-1",
    "href": "week_3.html#orthorectification-topographic-correction-1",
    "title": "3  Week 3",
    "section": "8.3 3.1.3 Orthorectification / Topographic Correction",
    "text": "8.3 3.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\n\n8.3.1 How:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\n\n\n8.3.2 Corrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed."
  },
  {
    "objectID": "week_3.html#radiometric-calibration-1",
    "href": "week_3.html#radiometric-calibration-1",
    "title": "3  Week 3",
    "section": "8.4 3.1.4 Radiometric Calibration",
    "text": "8.4 3.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\n\n8.4.1 How:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\n\n\n8.4.2 Corrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\n\n\n8.4.3 Summary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials."
  },
  {
    "objectID": "week_3.html#feathering-1",
    "href": "week_3.html#feathering-1",
    "title": "3  Week 3",
    "section": "9.1 3.2.1 Feathering",
    "text": "9.1 3.2.1 Feathering\n\n9.1.1 Data Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\n\n\n9.1.2 Feathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\n\n\n9.1.3 Practical\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area."
  },
  {
    "objectID": "week_3.html#image-enhancement-1",
    "href": "week_3.html#image-enhancement-1",
    "title": "3  Week 3",
    "section": "9.2 3.2.2 Image enhancement",
    "text": "9.2 3.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\n\n9.2.0.1 3.2.2.1 Contrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\n\n\n9.2.1 3.2.2.2 Ratio:\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\n\n9.2.1.1 Usage:\nFor instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\n\n\n9.2.1.2 Practical:\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\n\n9.2.2 3.2.2.3 Filtering:\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\n\n9.2.2.1 Usage:\nLow-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\n\n\n9.2.2.2 Practical:\nThe Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\n\n\n9.2.3 3.2.2.4 PCA:\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\n\n9.2.3.1 Usage:\nApplied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\n\n\n9.2.3.2 Practical:\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\n\n9.2.4 3.2.2.5 Texture:\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\n\n9.2.4.1 Usage:\nEnhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\n\n\n9.2.4.2 Practical:\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the GLCMTextures package can be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\n\n9.2.5 3.2.2.6 Fusion:\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\n\n9.2.5.1 Usage:\nIn Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\n\n\n9.2.5.2 Practical:\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data. In R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#content-summary",
    "href": "week_3.html#content-summary",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.1 3.1 Content summary:",
    "text": "3.1 3.1 Content summary:\nThis is an outline of what I have learnt this week and I will follow these outlines to summarise the knowledge or skills I have learnt:\n\n\n\n\n\nMind map for the summary outline\n\n3.1.1 3.1.1 Corrections\nBecause of the atmosphere, unevenness of the earth’s surface and other factors, remote sensing images are sometimes inaccurate, so we need to correct them. For example, as the scan line corrector on Landsat 7 failed, we need to correct the remotely sensed images.\nRemote sensed products are now often “corrected” to “Analysis Ready Data” (ARD), for instance, Landsat ARD products are processed through LEDPAS and L8SR algorithms to achieve surface reflectance standards, utilizing advanced algorithms and data processing techniques like LaSRC to provide users with processed surface reflectance data. However, when dealing with data not processed as ARD (e.g., very high resolution, drone data), understanding how the data was created remains crucial.\n\n3.1.1.1 3.1.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\nHow:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\nCorrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)\n\n\n3.1.1.2 3.1.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\nWhy is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\nCorrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance.\n\n\n3.1.1.3 3.1.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\nHow:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\nCorrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed.\n\n\n3.1.1.4 3.1.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\nHow:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\nCorrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\nSummary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials.\n\n\n\n\n3.1.2 3.1.2 Data joining and enhancement\n\n3.1.2.1 3.1.2.1 Feathering\nData Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\nFeathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\nPractical:\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area.\n\n\n3.1.2.2 3.1.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\nContrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\nRatio:\n\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\nUsage: For instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\nPractical:\n\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\nFiltering:\n\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\nUsage: Low-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\nPractical: The Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\nPCA:\n\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\nUsage: Applied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\nPractical:\n\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\nTexture:\n\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\nUsage: Enhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\nPractical:\n\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the ‘GLCMTextures’ package is able to be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\nFusion:\n\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\nUsage: In Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\nPractical:\n\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data.\nIn R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#applications-of-the-content",
    "href": "week_3.html#applications-of-the-content",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.2 3.2 Applications of the content:",
    "text": "3.2 3.2 Applications of the content:\nAs I mentioned in my summary, although many remote sensing image products have now been corrected by data providers, there are also images that are uncorrected or require more demanding corrections. According to Aasen et al. (2018), the article discusses sensor technology, measurement procedures, and data correction workflows (workflows graph as shown below (‘Empirical Line Correction’, which is covered in our lecture)) in UAV spectral remote sensing, including geometric processing, radiometric calibration, scene reflectance generation, and scene reflectance correction. These correction techniques help in accurately representing the energy reflected from the environment as pixels in a data product. The article provides a comprehensive evaluation of the state-of-the-art methods in UAV spectral remote sensing, incorporating over a decade of experimentation and operational demonstrations to offer clear guidance for acquiring reliable data. Despite providing detailed techniques and correction workflows, implementing these complex techniques and processes may be challenging for beginners and small project teams.\n\n\n\n\n\nData correction workflows (Aasen et al., 2018)\nRatio enhancement is widely used in reality and NDVI is a very popular application of ratio enhancement, so the application of NDVI is crucial. NDVI (Normalized Difference Vegetation Index) is a numerical indicator that uses the visible and near-infrared bands of the electromagnetic spectrum to assess vegetation health and density. It is calculated using the following formula:\n\n\n\n\n\nThere are several papers mentioning the application of NDVI, but certainly the application of NDVI has some disadvantages besides the advantages. Sims et al. (2014) uses NDVI as the main tool to assess vegetation responses to drought, finding that forest ecosystems maintain higher greenness compared to non-forest ecosystems under drought conditions, with little change in greenness even under extreme drought. In this essay, NDVI assists in improving predictions of water stress impacts on forest ecosystems at low spectral sensitivity levels by demonstrating differences in drought sensitivity among ecosystems (Sims et al., 2014). For example, predicting declines in carbon uptake in forest ecosystems by observing NDVI declines in adjacent ecosystems with high spectral sensitivity (Sims et al., 2014). NDVI could reveal differential responses of forest and non-forest ecosystems to drought, providing an effective tool for predicting and assessing carbon uptake in forest ecosystems under global climate change (Sims et al., 2014). However, NDVI may saturate in dense forest systems, limiting its ability to accurately reflect physiological responses of forest ecosystems under extreme drought conditions.\nIn addition, NDVI is utilized as a tool to monitor the performance and changes of certain strategic crops in Egypt throughout the growing season, showcasing a time series analysis of crop growth stages and vegetation health (Farg et al., 2019).\n\n\n\n\n\nThe application of NDVI on the crop (‘Different NDVI for June, July and September for the study area.’) (Farg et al., 2019)\nNDVI shows the capability for accurate crop classification under complex terrain conditions, enhancing the precision of crop discrimination through analysis of remote sensing data over different periods (Farg et al., 2019). Although time-series analysis based on NDVI is instrumental in understanding crop growth and health, it may not be sufficiently sensitive to the minute spectral variations among different crop types. This limitation is particularly pronounced in the initial phases of crop development, which could affect the precise identification and categorization of certain crop types."
  },
  {
    "objectID": "week_3.html#personal-reflection",
    "href": "week_3.html#personal-reflection",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.3 3.3 Personal reflection:",
    "text": "3.3 3.3 Personal reflection:\nThis week’s summary section is organised in a more textual way. Although I have studied remote sensing in my undergraduate studies and have worked with some of the applications of remotely sensing, we have only had a superficial understanding of image correction because many remote sensing imagery products have been corrected in their own right. In this week’s work, we covered a lot of knowledge and skills of image corrections and image enhancement, and I would like to make up for my shortcomings, so I’ve put together more of a basics section.\nAlthough the principles of the correction technique for remotely sensed images are difficult, the technique could be applied to high-resolution images taken by UAVs and is constantly being developed, which is probably one of the reasons why the technique is so attractive. Data correction is an early stage in the processing and analysis of remotely sensed data (and is needed anyway, even though much of the data that may be available to the user may have already been corrected by the data provider) and is very important and meaningful, as it is the foundation of all analyses related to remotely sensed data. If the individual works in a company related to remotely sensed image processing, then the correction of remotely sensed images may be a necessary element to master.\nBefore studying this course, image enhancement was something I was exposed to a lot, especially ratio enhancement. Image enhancement makes it easier to recognise and extract features of water bodies, plants and other features. This part is also very widely used in reality. In China, the Bureau of Natural Resources often uses image enhancement to survey natural resources such as forests and so on.\nOverall, I have learnt knowledge and skills in remotely sensed image product corrections and image joining and enhancement this week.This allows me to supplement the shortcomings of my undergraduate learning content to some extent, and enriches my knowledge, which may allow me to be more competitive in my future job."
  },
  {
    "objectID": "week_3.html#week-3---remote-sensing-data",
    "href": "week_3.html#week-3---remote-sensing-data",
    "title": "3  Week 3",
    "section": "3.1 Week 3 - Remote sensing data:",
    "text": "3.1 Week 3 - Remote sensing data:"
  },
  {
    "objectID": "week_3.html#reference-list",
    "href": "week_3.html#reference-list",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.4 3.4 Reference list",
    "text": "3.4 3.4 Reference list\nAasen, H., Honkavaara, E., Lucieer, A., & Zarco-Tejada, P. J. (2018) Quantitative Remote Sensing at Ultra-High Resolution with UAV Spectroscopy: A Review of Sensor Technology, Measurement Procedures, and Data Correction Workflows. Remote sensing (Basel, Switzerland). [Online] 10 (7), 1091-. https://doi.org/10.3390/rs10071091\nFarg, E., Ramadan, M. N., and Arafat, S. M. (2019) Classification of some strategic crops in Egypt using multi remotely sensing sensors and time series analysis. The Egyptian journal of remote sensing and space sciences. [Online] 22 (3), 263–270. https://doi.org/10.1016/j.ejrs.2019.07.002\nSims, D. A., Brzostek, E. R., Rahman, A. F., Dragoni, D., and Phillips, R. P. (2014) An improved approach for remotely sensing water stress impacts on forest C uptake. Global change biology. [Online] 20 (9), 2856–2866. https://doi.org/10.1111/gcb.12537"
  },
  {
    "objectID": "week_4.html#summary",
    "href": "week_4.html#summary",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.1 4.1 Summary",
    "text": "4.1 4.1 Summary\n\n4.1.1 4.1.1 Metropolitan - Shanghai\nShanghai is one of the largest cities in China and with a population of over 24 million, making it one of the most populous cities in the world. As the economic, transportation, technological and cultural centre of China, Shanghai is known for its unique metropolitan look and rapid modern development. The city is not only one of the world’s financial centres, but also demonstrates a perfect blend of tradition and modernity with its historical landmarks such as the Bund and the Shanghai Tower, which boasts one of the world’s tallest buildings.\n\n\n\n\n\nShanghai City View Image (source: Baidu)\nBelow is a map of the city of Shanghai, showing the city boundaries, major roads and river networks, and the geographic location of the city:\n\n\n\n\n\nShanghai Map (Pan et al., 2020)\nAs cities grow, cultivated land is occupied by artificial land, and food security and sustainable development are challenged. The following figure shows the change of land cover in Shanghai from 2000 to 2011 (Pan et al., 2020). In the figure below, we will find that as the city of Shanghai develops, the area of land changed from arable land to other land use types, especially artificial land, is much larger than the area of other land types changed to cultivated land. In short, arable land is decreasing rapidly. We needed a policy to address this issue.\n\n\n\n\n\n‘Land cover change map for Shanghai from 2000 to 2011’ (Pan et al., 2020)\n\n\n4.1.2 4.1.2 Policy\nThe State Council, the highest organ of state administration in the People’s Republic of China, has issued documents containing policy ‘Arable Land Minimum’ related to the protection of arable land, also known as the red line policy for arable land, which is aimed at solving the problem of basic arable land being occupied by artificial land use as cities are developing:\n\n“National General Land Use Planning Outline (2006-2020)”:\n\nBy 2010 and 2020, the total arable land area in Chinese Mainland is to be maintained at 121.2 million hectares (18.18 billion mu) and 120.33 million hectares (18.05 billion mu), respectively.\nEnsure that the area of basic farmland does not decrease from 104 million hectares (15.6 billion mu) during the planning period, with an improvement in quality.\nSupplement 63,000 hectares (945,000 mu) of arable land through land consolidation by 2020 and 182,000 hectares (2.73 million mu).\n\n“Adjustment Plan for the National General Land Use Planning Outline (2006-2020)” (2016):\n\nBy 2020, the total arable land area in Chinese Mainland is targeted to be 18.65 billion mu.\nEnsure that the national basic farmland area of 15.46 billion mu does not decrease during the planning period, with an improvement in quality.\n\n\nThe policy related to arable land in the above policy documents is uniformly referred to as ‘Arable Land Minimum’.\nThe ‘three zones and three lines’ policy (“three zones” refers to three types of national land space, namely, urban space, agricultural space, and ecological space; the “three lines” corresponds to the three control lines of urban development boundaries, permanent basic farmland, and ecological protection red lines drawn in urban space, agricultural space, and ecological space, respectively) issued by the State Council at the end of 2019 related to the policy of ‘Arable Land Minimum’. The guidance for implementation is being used today, which emphasises:\n\nImplement permanent special protection for arable land, establishing a permanent basic farmland system.\nRectify issues with already designated permanent basic farmland to ensure stability in area, quality, and layout.\n\nThe above is a summary of policy ‘Arable Land Minimum’ also known as the red line policy for arable land. Overall, the policy emphasises the protection of arable land and gives targets such as a bottom line for the area of arable land. For Shanghai, the Shanghai Municipal People’s Government has issued “Several Opinions on Comprehensively Promoting High-Quality Utilisation of Land Resources in the City”, in which the ‘Arable Land Minimum’ policy of arable land protection is also emphasised. In the implementation of this policy, it is essential to monitor and identify agricultural land, and this is relevant to the application of remotely sensing that we have learnt.\n\n\n4.1.3 4.1.3 How this links to global agendas / goals\nThe Chinese ‘Arable Land Minimum’ policy for arable land refers to maintaining the total amount of arable land at no less than a bottom-line standard to ensure national food security and sustainable development. This policy is closely linked to global agendas and goals, and in particular responds to the United Nations Sustainable Development Goals (SDGs).\n\nThe Chinese ‘Arable Land Minimum’ Policy, by ensuring a minimum amount of arable land, directly supports SDG 2’s goal to end hunger and enhance food security. This policy prevents the overdevelopment of farmland, safeguarding the basic resources for food production, thereby promoting stable food supply and sustainable agricultural development.\n\n\n\n\n\n\n‘Zero Hunger’ of SDGs Image (source: SDGs)"
  },
  {
    "objectID": "week_4.html#application",
    "href": "week_4.html#application",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.2 4.2 Application",
    "text": "4.2 4.2 Application\nTo implement the above policy, the identification and monitoring of arable land is crucial, where remotely sensed could play an essential role.\n\n4.2.1 4.2.1 remotely sensed data set\nAerial images:\n\nThe biggest advantage of aerial images over satellite images is that the digital surface model (DSM) could be constructed through three-dimensional image pairs, obtaining accurate height information of surface objects (including ground elevation data of the heights of buildings, bridges and trees, etc., which represents the most realistic ground undulation), providing data and technical support for the automated monitoring of illegal land use. Aerial imagery (using sensors carried on board aircraft to capture images) provides a resolution of decimetre level, which is higher than the satellite imagery (Tong, 2023).\n\nHigh temporal and spatial resolution remotely sensed images:\n\nThis refers to the smallest ground unit area that could be covered by remotely sensed data (spatial resolution) and the frequency of remotely sensed observations (temporal resolution). Remotely sensed data with a high spatial and temporal resolution provides detailed ground information and enables frequent monitoring of the same area, so that the details of changes in the area over time are captured.\n\nHyperspectral remotely sensed data:\n\nHyperspectral remotely sensed data focuses on capturing the reflectance or radiation properties of terrestrial objects over a wide spectral range, which is capable of acquiring data in hundreds or thousands of narrow bands. This allows hyperspectral remotely sensed data to be analysed in detail in terms of the chemical composition and physical state of objects.\n\n\n\n4.2.2 4.2.2 How could the data be applied to solve the policy challenge\nAerial images:\n\nDigital surface models (DSMs) are generally generated automatically by dense matching algorithms on aerial 3D imagery (Tong, 2023). In the application of monitoring changes in arable land, the DSMs of two time phases are differentiated to obtain the change spots (Tong, 2023). Then, the detection results are overlayed with the orthophotographs of the two time phases before and after, and the change spots caused by interference and noise are deleted, and the change detection results are overlayed and analysed with the arable land spots. If the change detection maps fall into the cultivated land spots, the attributes will be marked as suspected problematic spots (Tong, 2023). Finally, whether the cultivated land area has changed or not could be identified by the person in charge of the field survey or by combining with the high resolution aerial images.\nIn 2022, the Shanghai Surveying and Mapping Institute has achieved for the first time the acquisition of city-wide aerial laser scanning point cloud data (as shown in the figure below), and generated DSM data based on the point cloud data automatically, and the timeliness of the data acquisition is able to guarantee the demand of the inspector’s work (Tong, 2023).\n\n\n\n\n\n\nPoint cloud data (Tong, 2023)\nHigh temporal and spatial resolution remotely sensed images:\n\nHigh spatial and temporal resolution remotely sensed data are suitable for application scenarios that require the monitoring of rapidly changing events, such as the monitoring of urban development, the growth of crops, and the rapid response to natural disasters.\nXu et al. (2022) presents a method (as shown in the flowchart below) for the precise extraction of cultivated land information from remote sensing images using deep convolutional networks and geographical thematic scene division. It constructs a framework for fine extraction of large-area cultivated land parcels, employing an improved deep learning semantic segmentation network (DSCUnet) for image division, followed by an extended multi-channel richer convolutional features (RCF) network to refine the boundaries of cultivated land parcels from agricultural functional scenes (Xu et al., 2022). Using this method allows for accurate extraction of the boundaries of the cultivated land and thus monitoring the changes in the area of the cultivated land.\n\n\n\n\n\n\nFlowchart of the method for the precise extraction (Xu et al., 2022)\nHyperspectral remotely sensed data:\n\nHyperspectral remotely sensed data are widely used in fields such as vegetation classification, soil property analysis, rock and mineral identification, and water quality monitoring, because different substances and objects have unique spectral signatures, which allow hyperspectral remotely sensed data to accurately differentiate and identify these objects.\nIn some areas of China, arable land is ‘non-grain’ and most of the ‘non-grain’ arable land could not be identified with the human eyes (Tong, 2023). The satellite equipped with hyperspectral sensors provides continuous spectral information of each feature, and quantitatively analyses the material composition to complete the fine classification and identification of the features (Tong, 2023). Finally, the classification results obtained are superimposed on the maps of arable land to obtain the suspected areas of ” non-grain” of arable land (Tong, 2023). To some extent, this makes it possible to monitor arable land to ensure that crops are growing on it.\n\n\n\n4.2.3 4.2.3 Advantages and possible challenges of using these remotely sensed data\nAdvantages：\n\nThe advantages of using these remotely sensed data have been described in detail in both parts 4.2.1 and 4.2.2, so they will not be repeated in this part. Nevertheless, it is worth mentioning that the applications in 4.2.2 not only could be used in China, but also in other countries or regions of the world.\n\nPossible challenges:\n\nAerial images and Hyperspectral remotely sensed data:\n\nDSM data production costs are high, the accumulation of operational application processes and data processing models for hyperspectral remote sensing may be insufficient, and UAV technology needs to be improved in terms of flight range and endurance.\n\nHigh temporal and spatial resolution remotely sensed images:\n\nNeed for Post-processing: Since edge detection methods are sensitive to noise and may produce discontinuous lines, additional post-processing steps are required to ensure the generation of closed polygons, adding to the complexity of data processing (Xu et al., 2022).\nComplexity in Data Processing: High spatial resolution images involve large volumes of data, requiring more computational resources and time, especially pronounced when dealing with extensive areas.\nFeature Confusion: High spatial resolution images provide more detailed surface information, but this also means the boundaries between different land cover types (e.g., cultivated land and buildings, roads) might become blurred, increasing the difficulty of classification and segmentation.\nHigh Costs: Acquiring high spatial resolution remote sensing data tends to be expensive, particularly for large area coverage, where the cost issue becomes more pronounced.\n\nAccording to Dong et al. (2023), there are generally potential challenges associated with using remotely sensed data for monitoring arable land in the era of big data:\n\nAvailability and Consistency of Remote Sensing Data: Although satellite remotely sensing is a primary method for cultivated land monitoring, there is a rich diversity of data that comes with varying degrees of consistency and standardization. With the introduction of multiple sets of remotely sensed monitoring data, significant inconsistencies and uncertainties have emerged, which can complicate the monitoring process and analysis of cultivated land.\nIntegration of Scientific Research and Industry: While automated classification technologies have made significant strides in the research field, actual application scenarios often require extensive manual intervention or are predominantly manual. This gap highlights the need for deeper integration and interaction between scientific advancements and industry practices.\nData Sharing and Accessibility: Despite the considerable work done by natural resources departments, such as the national land use status surveys and cultivated land quality monitoring, there is a notable lack of open access to the raw data from these surveys. Enhancing data sharing and opening up access to these valuable datasets could foster more comprehensive and accurate monitoring and research."
  },
  {
    "objectID": "week_4.html#reflection",
    "href": "week_4.html#reflection",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.3 4.3 Reflection",
    "text": "4.3 4.3 Reflection\nThrough this week’s study, apart from the ‘Arable Land Minimum’ policy I have also looked up information on other policies, such as policies in the Chinese mainland that are related to the goal of ‘achieving peak carbon by 2030 and carbon neutrality by 2060’, and so on. I found it very interesting to apply my knowledge of remote sensing to possible scenarios of policy implementation, which made me feel that I had learnt a meaningful lesson.\nIn looking up what kind of remotely sensed data and technology would help with policy implementation, I learnt about aerial imagery, hyperspectral impacts, and high temporal and spatial resolution imagery, amongst other things I had heard about point cloud data before. This week’s self-study about remotely sensed data was something I had only heard about in class, but had not seen their in-depth applications. Reading through the literature especially the papers that I read this week that were published within three years allowes me to see cutting edge remotely sensing applications and makes me more interested in the applications of remote sensing technology. Because they are really cool and you are even able to analyse a lot of things that may be difficult to identify with the naked eyes through hyperspectral imagery.\nIn addition, I think that with the continuous upgrading of the UAV technology, the automatic generation of DSM data from point cloud data scanned by UAV-mounted sensors may become more and more widely used in the future. This is possible due to the convenience of capturing 3D data and the fact that the accuracy of the DSM data generated from this application would increase in the future as AI algorithms continue to evolve. Continuing to explore and learn more about this area on my own may be helpful in the future job."
  },
  {
    "objectID": "week_4.html#reference-list",
    "href": "week_4.html#reference-list",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.4 4.4 Reference list",
    "text": "4.4 4.4 Reference list\nDong, J., Cui, Y., Di, Y., Gao, X., Chen, X., Yang, L., Cai, Y., Ning, J. and Liu, J. (2023) Opportunities and challenges in monitoring cultivated land red line in big data era. Bulletin of Chinese Academy of Sciences. [Online] 38(12), 1781-1792. DOI:10.16418/j.issn.1000-3045.20230928001.\nPan, H., Tong, X., Xu, X., Luo, X., Jin, Y., Xie, H., and Li, B. (2020) Updating of land cover maps and change analysis using globeland30 product: A case study in Shanghai metropolitan area, China. Remote sensing (Basel, Switzerland). [Online] 12 (19), 1–25. DOI: 10.3390/rs12193147.\nTong, T. (2023) Application of Remote Sensing Technology in Supervision of Cultivated Land Protection. Modern Surveying and Mapping. [Online] 46(3), 30-33. DOI:10.3969/j.issn.1672-4097.2023.03.009.\nXu, L., Ming, D., Du, T., Chen, Y., Dong, D., and Zhou, C. (2022) Delineation of cultivated land parcels based on deep convolutional networks and geographical thematic scene division of remotely sensed images. Computers and electronics in agriculture. [Online] 192106611-. https://doi.org/10.1016/j.compag.2021.106611."
  },
  {
    "objectID": "week_5.html",
    "href": "week_5.html",
    "title": "5  Week 5",
    "section": "",
    "text": "No entry required for this week. Please see the learning diary of other weeks!"
  }
]