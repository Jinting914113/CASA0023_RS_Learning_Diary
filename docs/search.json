[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary - CASA0023",
    "section": "",
    "text": "About\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  week_2",
    "section": "",
    "text": "For my Xaringan presentation please click or copy the following link:\nhttps://jinting914113.github.io/CASA23_RS/\nNo entries for this week"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3",
    "section": "",
    "text": "4 Week 3 - Remote sensing data:"
  },
  {
    "objectID": "week_3.html#geometric-correction",
    "href": "week_3.html#geometric-correction",
    "title": "3  Week 3",
    "section": "6.1 3.1.1 Geometric Correction",
    "text": "6.1 3.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\n\n6.1.1 How:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\n\n\n6.1.2 Corrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)"
  },
  {
    "objectID": "week_3.html#atmospheric-correction",
    "href": "week_3.html#atmospheric-correction",
    "title": "3  Week 3",
    "section": "6.2 3.1.2 Atmospheric Correction",
    "text": "6.2 3.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\n\n6.2.1 Why is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\n\n\n6.2.2 Corrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance."
  },
  {
    "objectID": "week_3.html#orthorectification-topographic-correction",
    "href": "week_3.html#orthorectification-topographic-correction",
    "title": "3  Week 3",
    "section": "6.3 3.1.3 Orthorectification / Topographic Correction",
    "text": "6.3 3.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\n\n6.3.1 How:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\n\n\n6.3.2 Corrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed."
  },
  {
    "objectID": "week_3.html#radiometric-calibration",
    "href": "week_3.html#radiometric-calibration",
    "title": "3  Week 3",
    "section": "6.4 3.1.4 Radiometric Calibration",
    "text": "6.4 3.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\n\n6.4.1 How:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\n\n\n6.4.2 Corrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\n\n\n6.4.3 Summary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials."
  },
  {
    "objectID": "week_3.html#feathering",
    "href": "week_3.html#feathering",
    "title": "3  Week 3",
    "section": "7.1 3.2.1 Feathering",
    "text": "7.1 3.2.1 Feathering\n\n7.1.1 Data Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\n\n\n7.1.2 Feathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\n\n\n7.1.3 Practical\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area."
  },
  {
    "objectID": "week_3.html#image-enhancement",
    "href": "week_3.html#image-enhancement",
    "title": "3  Week 3",
    "section": "7.2 3.2.2 Image enhancement",
    "text": "7.2 3.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\n\n7.2.0.1 3.2.2.1 Contrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\n\n\n7.2.1 3.2.2.2 Ratio:\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\n\n7.2.1.1 Usage:\nFor instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\n\n\n7.2.1.2 Practical:\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\n\n7.2.2 3.2.2.3 Filtering:\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\n\n7.2.2.1 Usage:\nLow-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\n\n\n7.2.2.2 Practical:\nThe Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\n\n\n7.2.3 3.2.2.4 PCA:\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\n\n7.2.3.1 Usage:\nApplied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\n\n\n7.2.3.2 Practical:\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\n\n7.2.4 3.2.2.5 Texture:\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\n\n7.2.4.1 Usage:\nEnhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\n\n\n7.2.4.2 Practical:\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the GLCMTextures package can be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\n\n7.2.5 3.2.2.6 Fusion:\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\n\n7.2.5.1 Usage:\nIn Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\n\n\n7.2.5.2 Practical:\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data. In R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#geometric-correction-1",
    "href": "week_3.html#geometric-correction-1",
    "title": "3  Week 3",
    "section": "8.1 3.1.1 Geometric Correction",
    "text": "8.1 3.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\n\n8.1.1 How:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\n\n\n8.1.2 Corrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)"
  },
  {
    "objectID": "week_3.html#atmospheric-correction-1",
    "href": "week_3.html#atmospheric-correction-1",
    "title": "3  Week 3",
    "section": "8.2 3.1.2 Atmospheric Correction",
    "text": "8.2 3.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\n\n8.2.1 Why is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\n\n\n8.2.2 Corrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance."
  },
  {
    "objectID": "week_3.html#orthorectification-topographic-correction-1",
    "href": "week_3.html#orthorectification-topographic-correction-1",
    "title": "3  Week 3",
    "section": "8.3 3.1.3 Orthorectification / Topographic Correction",
    "text": "8.3 3.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\n\n8.3.1 How:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\n\n\n8.3.2 Corrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed."
  },
  {
    "objectID": "week_3.html#radiometric-calibration-1",
    "href": "week_3.html#radiometric-calibration-1",
    "title": "3  Week 3",
    "section": "8.4 3.1.4 Radiometric Calibration",
    "text": "8.4 3.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\n\n8.4.1 How:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\n\n\n8.4.2 Corrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\n\n\n8.4.3 Summary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials."
  },
  {
    "objectID": "week_3.html#feathering-1",
    "href": "week_3.html#feathering-1",
    "title": "3  Week 3",
    "section": "9.1 3.2.1 Feathering",
    "text": "9.1 3.2.1 Feathering\n\n9.1.1 Data Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\n\n\n9.1.2 Feathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\n\n\n9.1.3 Practical\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area."
  },
  {
    "objectID": "week_3.html#image-enhancement-1",
    "href": "week_3.html#image-enhancement-1",
    "title": "3  Week 3",
    "section": "9.2 3.2.2 Image enhancement",
    "text": "9.2 3.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\n\n9.2.0.1 3.2.2.1 Contrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\n\n\n9.2.1 3.2.2.2 Ratio:\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\n\n9.2.1.1 Usage:\nFor instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\n\n\n9.2.1.2 Practical:\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\n\n9.2.2 3.2.2.3 Filtering:\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\n\n9.2.2.1 Usage:\nLow-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\n\n\n9.2.2.2 Practical:\nThe Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\n\n\n9.2.3 3.2.2.4 PCA:\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\n\n9.2.3.1 Usage:\nApplied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\n\n\n9.2.3.2 Practical:\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\n\n9.2.4 3.2.2.5 Texture:\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\n\n9.2.4.1 Usage:\nEnhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\n\n\n9.2.4.2 Practical:\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the GLCMTextures package can be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\n\n9.2.5 3.2.2.6 Fusion:\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\n\n9.2.5.1 Usage:\nIn Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\n\n\n9.2.5.2 Practical:\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data. In R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#content-summary",
    "href": "week_3.html#content-summary",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.1 3.1 Content summary:",
    "text": "3.1 3.1 Content summary:\nThis is an outline of what I have learnt this week and I will follow these outlines to summarise the knowledge or skills I have learnt:\n\n\n\n\n\nMind map for the summary outline\n\n3.1.1 3.1.1 Corrections\nBecause of the atmosphere, unevenness of the earth’s surface and other factors, remote sensing images are sometimes inaccurate, so we need to correct them. For example, as the scan line corrector on Landsat 7 failed, we need to correct the remotely sensed images.\nRemote sensed products are now often “corrected” to “Analysis Ready Data” (ARD), for instance, Landsat ARD products are processed through LEDPAS and L8SR algorithms to achieve surface reflectance standards, utilizing advanced algorithms and data processing techniques like LaSRC to provide users with processed surface reflectance data. However, when dealing with data not processed as ARD (e.g., very high resolution, drone data), understanding how the data was created remains crucial.\n\n3.1.1.1 3.1.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\nHow:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\nCorrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)\n\n\n3.1.1.2 3.1.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\nWhy is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\nCorrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance.\n\n\n3.1.1.3 3.1.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\nHow:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\nCorrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed.\n\n\n3.1.1.4 3.1.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\nHow:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\nCorrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\nSummary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials.\n\n\n\n\n3.1.2 3.1.2 Data joining and enhancement\n\n3.1.2.1 3.1.2.1 Feathering\nData Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\nFeathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\nPractical:\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area.\n\n\n3.1.2.2 3.1.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\nContrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\nRatio:\n\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\nUsage: For instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\nPractical:\n\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\nFiltering:\n\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\nUsage: Low-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\nPractical: The Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\nPCA:\n\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\nUsage: Applied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\nPractical:\n\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\nTexture:\n\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\nUsage: Enhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\nPractical:\n\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the ‘GLCMTextures’ package is able to be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\nFusion:\n\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\nUsage: In Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\nPractical:\n\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data.\nIn R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#applications-of-the-content",
    "href": "week_3.html#applications-of-the-content",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.2 3.2 Applications of the content:",
    "text": "3.2 3.2 Applications of the content:\nAs I mentioned in my summary, although many remote sensing image products have now been corrected by data providers, there are also images that are uncorrected or require more demanding corrections. According to Aasen et al. (2018), the article discusses sensor technology, measurement procedures, and data correction workflows (workflows graph as shown below (‘Empirical Line Correction’, which is covered in our lecture)) in UAV spectral remote sensing, including geometric processing, radiometric calibration, scene reflectance generation, and scene reflectance correction. These correction techniques help in accurately representing the energy reflected from the environment as pixels in a data product. The article provides a comprehensive evaluation of the state-of-the-art methods in UAV spectral remote sensing, incorporating over a decade of experimentation and operational demonstrations to offer clear guidance for acquiring reliable data. Despite providing detailed techniques and correction workflows, implementing these complex techniques and processes may be challenging for beginners and small project teams.\n\n\n\n\n\nData correction workflows (Aasen et al., 2018)\nRatio enhancement is widely used in reality and NDVI is a very popular application of ratio enhancement, so the application of NDVI is crucial. NDVI (Normalized Difference Vegetation Index) is a numerical indicator that uses the visible and near-infrared bands of the electromagnetic spectrum to assess vegetation health and density. It is calculated using the following formula:\n\n\n\n\n\nThere are several papers mentioning the application of NDVI, but certainly the application of NDVI has some disadvantages besides the advantages. Sims et al. (2014) uses NDVI as the main tool to assess vegetation responses to drought, finding that forest ecosystems maintain higher greenness compared to non-forest ecosystems under drought conditions, with little change in greenness even under extreme drought. In this essay, NDVI assists in improving predictions of water stress impacts on forest ecosystems at low spectral sensitivity levels by demonstrating differences in drought sensitivity among ecosystems (Sims et al., 2014). For example, predicting declines in carbon uptake in forest ecosystems by observing NDVI declines in adjacent ecosystems with high spectral sensitivity (Sims et al., 2014). NDVI could reveal differential responses of forest and non-forest ecosystems to drought, providing an effective tool for predicting and assessing carbon uptake in forest ecosystems under global climate change (Sims et al., 2014). However, NDVI may saturate in dense forest systems, limiting its ability to accurately reflect physiological responses of forest ecosystems under extreme drought conditions.\nIn addition, NDVI is utilized as a tool to monitor the performance and changes of certain strategic crops in Egypt throughout the growing season, showcasing a time series analysis of crop growth stages and vegetation health (Farg et al., 2019).\n\n\n\n\n\nThe application of NDVI on the crop (‘Different NDVI for June, July and September for the study area.’) (Farg et al., 2019)\nNDVI shows the capability for accurate crop classification under complex terrain conditions, enhancing the precision of crop discrimination through analysis of remote sensing data over different periods (Farg et al., 2019). Although time-series analysis based on NDVI is instrumental in understanding crop growth and health, it may not be sufficiently sensitive to the minute spectral variations among different crop types. This limitation is particularly pronounced in the initial phases of crop development, which could affect the precise identification and categorization of certain crop types."
  },
  {
    "objectID": "week_3.html#personal-reflection",
    "href": "week_3.html#personal-reflection",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.3 3.3 Personal reflection:",
    "text": "3.3 3.3 Personal reflection:\nThis week’s summary section is organised in a more textual way. Although I have studied remote sensing in my undergraduate studies and have worked with some of the applications of remotely sensing, we have only had a superficial understanding of image correction because many remote sensing imagery products have been corrected in their own right. In this week’s work, we covered a lot of knowledge and skills of image corrections and image enhancement, and I would like to make up for my shortcomings, so I’ve put together more of a basics section.\nAlthough the principles of the correction technique for remotely sensed images are difficult, the technique could be applied to high-resolution images taken by UAVs and is constantly being developed, which is probably one of the reasons why the technique is so attractive. Data correction is an early stage in the processing and analysis of remotely sensed data (and is needed anyway, even though much of the data that may be available to the user may have already been corrected by the data provider) and is very important and meaningful, as it is the foundation of all analyses related to remotely sensed data. If the individual works in a company related to remotely sensed image processing, then the correction of remotely sensed images may be a necessary element to master.\nBefore studying this course, image enhancement was something I was exposed to a lot, especially ratio enhancement. Image enhancement makes it easier to recognise and extract features of water bodies, plants and other features. This part is also very widely used in reality. In China, the Bureau of Natural Resources often uses image enhancement to survey natural resources such as forests and so on.\nOverall, I have learnt knowledge and skills in remotely sensed image product corrections and image joining and enhancement this week.This allows me to supplement the shortcomings of my undergraduate learning content to some extent, and enriches my knowledge, which may allow me to be more competitive in my future job."
  },
  {
    "objectID": "week_3.html#week-3---remote-sensing-data",
    "href": "week_3.html#week-3---remote-sensing-data",
    "title": "3  Week 3",
    "section": "3.1 Week 3 - Remote sensing data:",
    "text": "3.1 Week 3 - Remote sensing data:"
  },
  {
    "objectID": "week_3.html#reference-list",
    "href": "week_3.html#reference-list",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.4 3.4 Reference list",
    "text": "3.4 3.4 Reference list\nAasen, H., Honkavaara, E., Lucieer, A., & Zarco-Tejada, P. J. (2018) Quantitative Remote Sensing at Ultra-High Resolution with UAV Spectroscopy: A Review of Sensor Technology, Measurement Procedures, and Data Correction Workflows. Remote sensing (Basel, Switzerland). [Online] 10 (7), 1091-. https://doi.org/10.3390/rs10071091\nFarg, E., Ramadan, M. N., and Arafat, S. M. (2019) Classification of some strategic crops in Egypt using multi remotely sensing sensors and time series analysis. The Egyptian journal of remote sensing and space sciences. [Online] 22 (3), 263–270. https://doi.org/10.1016/j.ejrs.2019.07.002\nSims, D. A., Brzostek, E. R., Rahman, A. F., Dragoni, D., and Phillips, R. P. (2014) An improved approach for remotely sensing water stress impacts on forest C uptake. Global change biology. [Online] 20 (9), 2856–2866. https://doi.org/10.1111/gcb.12537"
  },
  {
    "objectID": "week_4.html#summary",
    "href": "week_4.html#summary",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.1 4.1 Summary",
    "text": "4.1 4.1 Summary\n\n4.1.1 4.1.1 Metropolitan - Shanghai\nShanghai is one of the largest cities in China and with a population of over 24 million, making it one of the most populous cities in the world. As the economic, transportation, technological and cultural centre of China, Shanghai is known for its unique metropolitan look and rapid modern development. The city is not only one of the world’s financial centres, but also demonstrates a perfect blend of tradition and modernity with its historical landmarks such as the Bund and the Shanghai Tower, which boasts one of the world’s tallest buildings.\n\n\n\n\n\nShanghai City View Image (source: Baidu)\nBelow is a map of the city of Shanghai, showing the city boundaries, major roads and river networks, and the geographic location of the city:\n\n\n\n\n\nShanghai Map (Pan et al., 2020)\nAs cities grow, cultivated land is occupied by artificial land, and food security and sustainable development are challenged. The following figure shows the change of land cover in Shanghai from 2000 to 2011 (Pan et al., 2020). In the figure below, we will find that as the city of Shanghai develops, the area of land changed from arable land to other land use types, especially artificial land, is much larger than the area of other land types changed to cultivated land. In short, arable land is decreasing rapidly. We needed a policy to address this issue.\n\n\n\n\n\n‘Land cover change map for Shanghai from 2000 to 2011’ (Pan et al., 2020)\n\n\n4.1.2 4.1.2 Policy\nThe State Council, the highest organ of state administration in the People’s Republic of China, has issued documents containing policy ‘Arable Land Minimum’ related to the protection of arable land, also known as the red line policy for arable land, which is aimed at solving the problem of basic arable land being occupied by artificial land use as cities are developing:\n\n“National General Land Use Planning Outline (2006-2020)”:\n\nBy 2010 and 2020, the total arable land area in Chinese Mainland is to be maintained at 121.2 million hectares (18.18 billion mu) and 120.33 million hectares (18.05 billion mu), respectively.\nEnsure that the area of basic farmland does not decrease from 104 million hectares (15.6 billion mu) during the planning period, with an improvement in quality.\nSupplement 63,000 hectares (945,000 mu) of arable land through land consolidation by 2020 and 182,000 hectares (2.73 million mu).\n\n“Adjustment Plan for the National General Land Use Planning Outline (2006-2020)” (2016):\n\nBy 2020, the total arable land area in Chinese Mainland is targeted to be 18.65 billion mu.\nEnsure that the national basic farmland area of 15.46 billion mu does not decrease during the planning period, with an improvement in quality.\n\n\nThe policy related to arable land in the above policy documents is uniformly referred to as ‘Arable Land Minimum’.\nThe ‘three zones and three lines’ policy (“three zones” refers to three types of national land space, namely, urban space, agricultural space, and ecological space; the “three lines” corresponds to the three control lines of urban development boundaries, permanent basic farmland, and ecological protection red lines drawn in urban space, agricultural space, and ecological space, respectively) issued by the State Council at the end of 2019 related to the policy of ‘Arable Land Minimum’. The guidance for implementation is being used today, which emphasises:\n\nImplement permanent special protection for arable land, establishing a permanent basic farmland system.\nRectify issues with already designated permanent basic farmland to ensure stability in area, quality, and layout.\n\nThe above is a summary of policy ‘Arable Land Minimum’ also known as the red line policy for arable land. Overall, the policy emphasises the protection of arable land and gives targets such as a bottom line for the area of arable land. For Shanghai, the Shanghai Municipal People’s Government has issued “Several Opinions on Comprehensively Promoting High-Quality Utilisation of Land Resources in the City”, in which the ‘Arable Land Minimum’ policy of arable land protection is also emphasised. In the implementation of this policy, it is essential to monitor and identify agricultural land, and this is relevant to the application of remotely sensing that we have learnt.\n\n\n4.1.3 4.1.3 How this links to global agendas / goals\nThe Chinese ‘Arable Land Minimum’ policy for arable land refers to maintaining the total amount of arable land at no less than a bottom-line standard to ensure national food security and sustainable development. This policy is closely linked to global agendas and goals, and in particular responds to the United Nations Sustainable Development Goals (SDGs).\n\nThe Chinese ‘Arable Land Minimum’ Policy, by ensuring a minimum amount of arable land, directly supports SDG 2’s goal to end hunger and enhance food security. This policy prevents the overdevelopment of farmland, safeguarding the basic resources for food production, thereby promoting stable food supply and sustainable agricultural development.\n\n\n\n\n\n\n‘Zero Hunger’ of SDGs Image (source: SDGs)"
  },
  {
    "objectID": "week_4.html#application",
    "href": "week_4.html#application",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.2 4.2 Application",
    "text": "4.2 4.2 Application\nTo implement the above policy, the identification and monitoring of arable land is crucial, where remotely sensed could play an essential role.\n\n4.2.1 4.2.1 remotely sensed data set\nAerial images:\n\nThe biggest advantage of aerial images over satellite images is that the digital surface model (DSM) could be constructed through three-dimensional image pairs, obtaining accurate height information of surface objects (including ground elevation data of the heights of buildings, bridges and trees, etc., which represents the most realistic ground undulation), providing data and technical support for the automated monitoring of illegal land use. Aerial imagery (using sensors carried on board aircraft to capture images) provides a resolution of decimetre level, which is higher than the satellite imagery (Tong, 2023).\n\nHigh temporal and spatial resolution remotely sensed images:\n\nThis refers to the smallest ground unit area that could be covered by remotely sensed data (spatial resolution) and the frequency of remotely sensed observations (temporal resolution). Remotely sensed data with a high spatial and temporal resolution provides detailed ground information and enables frequent monitoring of the same area, so that the details of changes in the area over time are captured.\n\nHyperspectral remotely sensed data:\n\nHyperspectral remotely sensed data focuses on capturing the reflectance or radiation properties of terrestrial objects over a wide spectral range, which is capable of acquiring data in hundreds or thousands of narrow bands. This allows hyperspectral remotely sensed data to be analysed in detail in terms of the chemical composition and physical state of objects.\n\n\n\n4.2.2 4.2.2 How could the data be applied to solve the policy challenge\nAerial images:\n\nDigital surface models (DSMs) are generally generated automatically by dense matching algorithms on aerial 3D imagery (Tong, 2023). In the application of monitoring changes in arable land, the DSMs of two time phases are differentiated to obtain the change spots (Tong, 2023). Then, the detection results are overlayed with the orthophotographs of the two time phases before and after, and the change spots caused by interference and noise are deleted, and the change detection results are overlayed and analysed with the arable land spots. If the change detection maps fall into the cultivated land spots, the attributes will be marked as suspected problematic spots (Tong, 2023). Finally, whether the cultivated land area has changed or not could be identified by the person in charge of the field survey or by combining with the high resolution aerial images.\nIn 2022, the Shanghai Surveying and Mapping Institute has achieved for the first time the acquisition of city-wide aerial laser scanning point cloud data (as shown in the figure below), and generated DSM data based on the point cloud data automatically, and the timeliness of the data acquisition is able to guarantee the demand of the inspector’s work (Tong, 2023).\n\n\n\n\n\n\nPoint cloud data (Tong, 2023)\nHigh temporal and spatial resolution remotely sensed images:\n\nHigh spatial and temporal resolution remotely sensed data are suitable for application scenarios that require the monitoring of rapidly changing events, such as the monitoring of urban development, the growth of crops, and the rapid response to natural disasters.\nXu et al. (2022) presents a method (as shown in the flowchart below) for the precise extraction of cultivated land information from remote sensing images using deep convolutional networks and geographical thematic scene division. It constructs a framework for fine extraction of large-area cultivated land parcels, employing an improved deep learning semantic segmentation network (DSCUnet) for image division, followed by an extended multi-channel richer convolutional features (RCF) network to refine the boundaries of cultivated land parcels from agricultural functional scenes (Xu et al., 2022). Using this method allows for accurate extraction of the boundaries of the cultivated land and thus monitoring the changes in the area of the cultivated land.\n\n\n\n\n\n\nFlowchart of the method for the precise extraction (Xu et al., 2022)\nHyperspectral remotely sensed data:\n\nHyperspectral remotely sensed data are widely used in fields such as vegetation classification, soil property analysis, rock and mineral identification, and water quality monitoring, because different substances and objects have unique spectral signatures, which allow hyperspectral remotely sensed data to accurately differentiate and identify these objects.\nIn some areas of China, arable land is ‘non-grain’ and most of the ‘non-grain’ arable land could not be identified with the human eyes (Tong, 2023). The satellite equipped with hyperspectral sensors provides continuous spectral information of each feature, and quantitatively analyses the material composition to complete the fine classification and identification of the features (Tong, 2023). Finally, the classification results obtained are superimposed on the maps of arable land to obtain the suspected areas of ” non-grain” of arable land (Tong, 2023). To some extent, this makes it possible to monitor arable land to ensure that crops are growing on it.\n\n\n\n4.2.3 4.2.3 Advantages and possible challenges of using these remotely sensed data\nAdvantages：\n\nThe advantages of using these remotely sensed data have been described in detail in both parts 4.2.1 and 4.2.2, so they will not be repeated in this part. Nevertheless, it is worth mentioning that the applications in 4.2.2 not only could be used in China, but also in other countries or regions of the world.\n\nPossible challenges:\n\nAerial images and Hyperspectral remotely sensed data:\n\nDSM data production costs are high, the accumulation of operational application processes and data processing models for hyperspectral remote sensing may be insufficient, and UAV technology needs to be improved in terms of flight range and endurance.\n\nHigh temporal and spatial resolution remotely sensed images:\n\nNeed for Post-processing: Since edge detection methods are sensitive to noise and may produce discontinuous lines, additional post-processing steps are required to ensure the generation of closed polygons, adding to the complexity of data processing (Xu et al., 2022).\nComplexity in Data Processing: High spatial resolution images involve large volumes of data, requiring more computational resources and time, especially pronounced when dealing with extensive areas.\nFeature Confusion: High spatial resolution images provide more detailed surface information, but this also means the boundaries between different land cover types (e.g., cultivated land and buildings, roads) might become blurred, increasing the difficulty of classification and segmentation.\nHigh Costs: Acquiring high spatial resolution remote sensing data tends to be expensive, particularly for large area coverage, where the cost issue becomes more pronounced.\n\nAccording to Dong et al. (2023), there are generally potential challenges associated with using remotely sensed data for monitoring arable land in the era of big data:\n\nAvailability and Consistency of Remote Sensing Data: Although satellite remotely sensing is a primary method for cultivated land monitoring, there is a rich diversity of data that comes with varying degrees of consistency and standardization. With the introduction of multiple sets of remotely sensed monitoring data, significant inconsistencies and uncertainties have emerged, which can complicate the monitoring process and analysis of cultivated land.\nIntegration of Scientific Research and Industry: While automated classification technologies have made significant strides in the research field, actual application scenarios often require extensive manual intervention or are predominantly manual. This gap highlights the need for deeper integration and interaction between scientific advancements and industry practices.\nData Sharing and Accessibility: Despite the considerable work done by natural resources departments, such as the national land use status surveys and cultivated land quality monitoring, there is a notable lack of open access to the raw data from these surveys. Enhancing data sharing and opening up access to these valuable datasets could foster more comprehensive and accurate monitoring and research."
  },
  {
    "objectID": "week_4.html#reflection",
    "href": "week_4.html#reflection",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.3 4.3 Reflection",
    "text": "4.3 4.3 Reflection\nThrough this week’s study, apart from the ‘Arable Land Minimum’ policy I have also looked up information on other policies, such as policies in the Chinese mainland that are related to the goal of ‘achieving peak carbon by 2030 and carbon neutrality by 2060’, and so on. I found it very interesting to apply my knowledge of remote sensing to possible scenarios of policy implementation, which made me feel that I had learnt a meaningful lesson.\nIn looking up what kind of remotely sensed data and technology would help with policy implementation, I learnt about aerial imagery, hyperspectral impacts, and high temporal and spatial resolution imagery, amongst other things I had heard about point cloud data before. This week’s self-study about remotely sensed data was something I had only heard about in class, but had not seen their in-depth applications. Reading through the literature especially the papers that I read this week that were published within three years allowes me to see cutting edge remotely sensing applications and makes me more interested in the applications of remote sensing technology. Because they are really cool and you are even able to analyse a lot of things that may be difficult to identify with the naked eyes through hyperspectral imagery.\nIn addition, I think that with the continuous upgrading of the UAV technology, the automatic generation of DSM data from point cloud data scanned by UAV-mounted sensors may become more and more widely used in the future. This is possible due to the convenience of capturing 3D data and the fact that the accuracy of the DSM data generated from this application would increase in the future as AI algorithms continue to evolve. Continuing to explore and learn more about this area on my own may be helpful in the future job."
  },
  {
    "objectID": "week_4.html#reference-list",
    "href": "week_4.html#reference-list",
    "title": "4  Week 4 - The Policy of ‘Arable Land Minimum’ in Shanghai",
    "section": "4.4 4.4 Reference list",
    "text": "4.4 4.4 Reference list\nDong, J., Cui, Y., Di, Y., Gao, X., Chen, X., Yang, L., Cai, Y., Ning, J. and Liu, J. (2023) Opportunities and challenges in monitoring cultivated land red line in big data era. Bulletin of Chinese Academy of Sciences. [Online] 38(12), 1781-1792. DOI:10.16418/j.issn.1000-3045.20230928001.\nPan, H., Tong, X., Xu, X., Luo, X., Jin, Y., Xie, H., and Li, B. (2020) Updating of land cover maps and change analysis using globeland30 product: A case study in Shanghai metropolitan area, China. Remote sensing (Basel, Switzerland). [Online] 12 (19), 1–25. DOI: 10.3390/rs12193147.\nTong, T. (2023) Application of Remote Sensing Technology in Supervision of Cultivated Land Protection. Modern Surveying and Mapping. [Online] 46(3), 30-33. DOI:10.3969/j.issn.1672-4097.2023.03.009.\nXu, L., Ming, D., Du, T., Chen, Y., Dong, D., and Zhou, C. (2022) Delineation of cultivated land parcels based on deep convolutional networks and geographical thematic scene division of remotely sensed images. Computers and electronics in agriculture. [Online] 192106611-. https://doi.org/10.1016/j.compag.2021.106611."
  },
  {
    "objectID": "week_5.html",
    "href": "week_5.html",
    "title": "5  Week 5",
    "section": "",
    "text": "No entry required for this week. Please see the learning diary of other weeks!"
  },
  {
    "objectID": "week_6.html#summary",
    "href": "week_6.html#summary",
    "title": "6  Week 6 - Intro to GEE",
    "section": "6.1 6.1 Summary:",
    "text": "6.1 6.1 Summary:\nGoogle Earth Engine (GEE) offers a platform enabling users to perform geospatial analysis on Google’s infrastructure. Users could interact with this platform through multiple ways. The Code Editor, a web-based integrated development environment (IDE), allows for script writing and running. Meanwhile, the Explorer offers a simplified web application for navigating the data catalog and.running basic analyses. Additionally, the client libraries provide Python and JavaScript wrappers around their web API (refernce source: GEE).\n\n6.1.1 6.1.1 The set up of GEE\nThis is a mind map summarising this section and covering the knowledge collated:\n\n\n\n\n\nThe set up of GEE mind map\n\n\n6.1.2 6.1.2 GEE in action (how we use it)\nWhat I think should be remembered on using GEE is this figure, which contains the location of each interactive action button:\n\n\n\n\n\nCode editor of GEE (source: CASA0023)\nOverall, what typical processes can we do in GEE? Please see this mind map below:\n\n\n\n\n\nMind map of what typical processes we could do in GEE\nIn this week, we covered what we could do in GEE (click the link to review the knowledge or code):\n\nLoading image collections\nReducing images (by region or regions or neighbourhoods)\nRegression\nJoins and filtering\n\n\n\n6.1.3 6.1.3 Summary of practical for this week\nThis week’s practical focused on the basics of using Google Earth Engine (GEE), including how to construct points, load and process Landsat image data on GEE, as well as how to perform image clipping, texture analysis, and Principal Component Analysis (PCA). Finally, we also learned exporting data and calculating NDVI in GEE.\nUsing GEE has allowed me to get the results of various analysis much more quickly, especially the texture enhancement and PCA analysis much faster than week three. Below are some of the results I’ve extracted from following the tutorials in practical:\n\n\n\n\n\nTexture\n\n\n\n\n\nPCA band 1 and 2\nI had partly forgotten about the analysis of the bright places in the image obtained by texture enhancement, so here’s a recap:\n\nIn remote sensing texture analysis, bright areas usually indicate regions of high texture variation or contrast. This may mean:\n\nComplex surface textures: For example, urban areas, forests, or agricultural fields, where the surface texture varies significantly due to different land features.\nHigh texture contrast areas: Compared to surrounding environments, these regions exhibit rapid spatial changes in surface characteristics, showing high-contrast texture features.\nChanges in land features or geomorphology: such as the boundaries between bare soil and vegetated areas, where texture analysis can reveal details of these geomorphological changes."
  },
  {
    "objectID": "week_6.html#application",
    "href": "week_6.html#application",
    "title": "6  Week 6 - Intro to GEE",
    "section": "6.2 6.2 Application:",
    "text": "6.2 6.2 Application:\nFirstly, GEE has a very wide range of applications and could be used in the areas described in the image below (although it may have been used by someone else I think the image still sums up the wide range of applications for GEE quite well):\n\n\n\n\n\nThe figure of GEE Applications (Amani et al., 2020)\nI think that NDVI has a wide range of applications even though it is a relatively common method of ratio enhancement in remotely sensing. In fact, the characteristics of GEE as a cloud computing platform could be seen from the academic research related to GEE and NDVI. That is, it makes it possible to calculate and analyse NDVI for large scale and long time series data:\n\nRuifeng et al. (2023) used the Google Earth Engine (GEE) platform and Landsat Normalized Difference Vegetation Index (NDVI) datasets, employing the Otsu algorithm to extract the spatial extent of limestone mining areas in Qingzhou City, Shandong Province, from 1985 to 2020. The research identified that the mining areas were primarily located in Shaozhuang Town, Wangfu Street, and the northern part of Miaozi Town, expanding from 2.0 km² in 1985 to 19.5 km² in 2020, more than a tenfold increase (Ruifeng et al., 2023). The advantage of utilizing NDVI and the GEE platform is the ability to efficiently process large datasets and rapidly identify overall changes in vegetation cover. However, its limitation lies in the inability to analyze the causes of those changes, merely highlighting areas where there has been a general increase or decrease in vegetation cover.\nMandal and Hosaka (2020) utilized the Google Earth Engine (GEE) platform and Landsat satellite images to assess the impact of cyclone disturbances on the Sundarbans mangrove forests in Bangladesh and India by evaluating changes in the Normalized Difference Vegetation Index (NDVI) before and after 21 cyclones between 1988 and 2016. The results showed that the affected forest area ranged from 0.5% to 24.1%, with the 2007 Sidr cyclone and a 1988 cyclone affecting 24.1% and 20.4% of the forest area, respectively (Mandal and Hosaka, 2020). The study found a significant positive correlation between cyclone maximum wind speed and the affected forest area, especially when the wind speed exceeded 101.9 km/h, significantly increasing the impact on forests (Mandal and Hosaka, 2020). The advantage of using NDVI and the GEE platform for analysis lies in the ability to process large datasets over long time series efficiently, providing a rapid and effective assessment of cyclone impacts on forests. However, this method is limited in that it captures changes in vegetation cover but cannot directly reflect the physical damage to tree structures caused by cyclones, or other ecological effects triggered by cyclones."
  },
  {
    "objectID": "week_6.html#reflection",
    "href": "week_6.html#reflection",
    "title": "6  Week 6 - Intro to GEE",
    "section": "6.3 6.3 Reflection:",
    "text": "6.3 6.3 Reflection:\nThe long awaited Google Earth Engine section has arrived! Actually, I have used GEE in my undergraduate dissertation and have also attended GEE trainings for representatives from various Chinese universities at a Chinese academic conference. At that time I have deeply felt the powerfulness of GEE because of its cloud computing attribute. A lot of work that cannot be done on a PC is possible with GEE, especially for analysing and researching large geographical areas and long time series. And I think GEE is much easier to use than ENVI or SNAP, as I only need to enter the code. For the same task, GEE is much faster than a PC.\nIt’s been more than a year since I’ve used GEE, so I’ve made a mind map to summarise and organise the content of this week’s lessons to make it easier for me to review later. I reviewed the basic operations of GEE such as loading and exporting, and I have also learnt how to do texture, PCA and other image enhancement techniques in GEE, which is very beneficial. Through reading the literature it has also been demonstrated that GEE works well for processing and analysing long time series and large scale data.\nThe power of GEE goes far beyond this week’s content, machine learning algorithms and other advanced algorithmic applications analysis could also be done with the help of GEE, which I am very interested in, and I look forward to exploring and learning more deeply about GEE next week!\nIt occurs to me that there is also a geographic cloud computing platform being developed by a company in China. In China, the geographic cloud computing platform now available is developed by a Chinese company called ‘PIESAT’, which is called ‘PIE-Engine’ where you are able to do many same things as GEE. In fact, the senior vice president of their company is a visiting professor to my undergraduate major, so I know the existence of ‘PIE-Engine’. I’m sure it would be very interesting to explore ‘PIE-Engine’ while studying GEE."
  },
  {
    "objectID": "week_6.html#reference-list",
    "href": "week_6.html#reference-list",
    "title": "6  Week 6 - Intro to GEE",
    "section": "6.4 6.4 Reference list",
    "text": "6.4 6.4 Reference list\nAmani, M., Ghorbanian, A., Ahmadi, S. A., Kakooei, M., Moghimi, A., Mirmazloumi, S. M., Moghaddam, S. H. A., Mahdavi, S., Ghahremanloo, M., Parsian, S., Wu, Q., and Brisco, B. (2020) Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review. IEEE journal of selected topics in applied earth observations and remote sensing. [Online] 135326–5350. DOI: 10.1109/JSTARS.2020.3021052\nMandal, M. S. H. and Hosaka, T. (2020) Assessing cyclone disturbances (1988–2016) in the Sundarbans mangrove forests using Landsat and Google Earth Engine. Natural hazards (Dordrecht). [Online] 102 (1), 133–150. DOI: 10.1007/s11069-020-03914-z\nRuifeng, L., Kai, Y., Xing, L., Xiaoli, L., Xitao, Z., Xiaocheng, G., Juan, F., and Shixin, C. (2023) Extraction and spatiotemporal changes of open-pit mines during 1985–2020 using Google Earth Engine: A case study of Qingzhou City, Shandong Province, China. Environmental monitoring and assessment. [Online] 195 (1), 209–209. DOI: 10.1007/s10661-022-10837-8"
  },
  {
    "objectID": "week_7.html#summary",
    "href": "week_7.html#summary",
    "title": "7  Week 7 - Classification I",
    "section": "7.1 7.1 Summary",
    "text": "7.1 7.1 Summary\n\n7.1.1 7.1.1 Review of how classified data is used:\nThe following mind map summarises the application areas of classified data that were covered in the lecture and summarises some data that is available for classified research or sensors that are the source of these classified data in their respective fields:\n\n\n\n\n\nhow classified data is used\n\n\n7.1.2 7.1.2 How to classify remotely sensed data\nClassification methods for remotely sensed data could be divided into unsupervised and supervised classification, more specifically see the mind map below:\n\n\n\n\n\nThe mind map of classification methods for remotely sensed data\nFor the summary of key concepts covered in the lecture:\n\nClassification and regression trees (CART):\n\nClassification and regression trees (CART) are a machine learning technique used to divide data into two or more discrete categories or to predict continuous variables. CART builds decision trees by recursively splitting the dataset into purer or less variable subsets. The evaluation of CART classification outcomes can be conducted by calculating the Gini impurity of leaves, with lower Gini impurity indicating better classification performance and purer leaves. (Avoid overfitting!)\n\nRandom Forests:\n\nRandom Forest classification is an ensemble learning method that enhances classification accuracy by building multiple decision trees and combining their predictions. The accuracy of Random Forest classification is commonly evaluated using the Out-of-Bag (OOB) error rate, with a lower OOB error indicating higher predictive accuracy.\n\nMaximum Likelihood:\n\nMaximum likelihood is a classification approach based on probability that determines the classification of each pixel by calculating its probability of belonging to various land cover types. It employs a multivariate normal distribution function to process image data, choosing the category with the highest probability as the pixel’s classification. This method allows for the consideration of prior probability information, although such information is often unavailable.\n\nSupport Vector Machines (SVM):\n\nIt is a binary linear classifier that categorises data by maximizing the margin between two classes of training data. When data are not linearly separable, SVM employs the kernel trick to find the optimal separating hyperplane by transforming the data into a higher dimensional space. The choice of parameters, such as C and Gamma, significantly impacts the model’s performance and is typically determined through grid search for optimal values.\n\n\n\n\n7.1.3 7.1.3 Practical\nThis week’s practical focused on classifying surface features using remote sensing data, including how to load and process Global Administrative Unit Layers (GAUL) data, and how to apply supervised learning methods for information extraction. Specifically, it explored using CART and Random Forest classifiers to classify Sentinel-2 satellite data of the Shenzhen area, and further discussed the classification accuracy assessment and the error matrix.\nIn the following section, I will mainly focus on summarising what I have been doing within Practical in terms of focusing on CART and Random Forest Classification:\nImage data after clipping using GEE (to enable comparison of classification results later on, so added to the learning diary):\n\n\n\n\n\nShenzhen Remotely Sensed Image\nIn the initial labelling of the training samples, I labelled too many pixel points beyond what GEE was running at so it produced errors. Therefore the number of labelled points must not exceed the maximum of 5000 pixel points required by Google. Later on, some of my labelled pixels were too few resulting in inaccurate classification results. Therefore, it is important to have the appropriate amount of labelled sample pixels.\nThis is an image of the CART classification result where I just began to have a significant error due to including too few pixels in the labelled samples:\n\n\n\n\n\nThe image of the CART classification results (with relatively large errors)\nFinally, the result obtained from the CART classification after I added some more labelled pixels is much better when comparing the result with the clipping Shenzhen satellite image mentioned in the previous section and the result of the first CART classification:\n\n\n\n\n\nThe image of the CART classification results (much better one)\nThe following image is the result of random forest classification. When we compare it with the cropped Shenzhen satellite image and all the results obtained from the previous CART classification, we could find that the results obtained from the Random Forest classification look the best to naked eyes (e.g., the classification for water is better and so on):\n\n\n\n\n\nThe image of the Random Forest classification results\nAccuracy assessment for the randomised forest classification result:\n\nRandom Forest Out of Bag (OOB) Error Estimate:\nThe Out of Bag (OOB) Error Estimate serves as a prediction error metric for random forest. This estimate is calculated using the data not employed in the training of each decision tree within the ensemble, known as the OOB sample. Through predicting outcomes with this data, the overall error rate is derived. In this specific case, the OOB error estimate is reported to be 0.007407407407407408, implying an extremely accurate prediction capability of the random forest classifier, closely aligning with a high accuracy level.\nResubstitution Accuracy:\nResubstitution accuracy is assessed by comparing the original training dataset with the model’s predictions. This accuracy metric reflects the model’s capability to correctly predict the outcomes of the data on which it was trained. Here, the resubstitution accuracy is 100%, indicating that the model perfectly predicts the training data outcomes.\nConfusion Matrix and Validation Accuracy:\nThe confusion matrix is a crucial tool for assessing a classification model’s performance on a dataset where the true values are known. It contrasts the actual target values with the model’s predictions, providing insights into both the accuracy and the types of errors the model makes. The validation accuracy derived from the confusion matrix in this analysis is 97.35449735449735%. This high accuracy level demonstrates the model’s effectiveness on the validation dataset, which was not used in its training. Additionally, the consumer accuracy detailed in the confusion matrix validates the model’s high predictive performance across various classes, with values such as 1 for several classes and 0.9393939393939394 and 0.8928571428571429 for others, reflecting nuanced predictive capabilities."
  },
  {
    "objectID": "week_7.html#application",
    "href": "week_7.html#application",
    "title": "7  Week 7 - Classification I",
    "section": "7.2 7.2 Application",
    "text": "7.2 7.2 Application\nThe following is the research on the use of CART classification in the GEE platform:\n\nPande et al. (2024) use the Classification and Regression Tree (CART) model in combination with the Google Earth Engine (GEE) to assess the impact of winter season’s land cover and change detection mapping on evapotranspiration (crop water requirement) parameters. It finds that using the CART model alongside GEE’s cloud computing proves to be an effective approach for precise land cover classification and change detection mapping, enabling the creation of pixel-based information for the study area during winter seasons (Pande et al., 2024).\nThe advantage of using the Classification and Regression Tree (CART) for land cover classification and change detection lies in its ability to handle complex datasets by recursively partitioning the input space based on predictive variables, creating a decision tree structure suitable for analyzing land cover dynamics. However, a limitation of the CART model is its potential for overfitting, especially in scenarios involving large and complex datasets.\n\nThe following is the research using Random Forest Classification on the GEE platform:\n\nNoi Phan et al. (2020) utilized the Random Forest (RF) classifier and Google Earth Engine (GEE) to investigate the impact of image composition methods on land cover classification accuracy. It shows that the choice of image composition strategies significantly affects accuracy, with the highest accuracy achieved by the summer season (June to September) time series datasets (Noi Phan et al., 2020). The advantages of using Random Forest for land cover classification include its robustness to noise and outliers, ability to handle high-dimensional and multi-source datasets, and generally higher accuracy compared to other popular classifiers such as SVM, kNN, or MLC (Noi Phan et al., 2020). Despite having fewer parameters compared to other machine learning algorithms, tuning parameters such as the number of trees (ntree) and the number of features to consider at each split (mtry) is still necessary. Inappropriate parameter settings may affect the model’s performance and generalizability.\nTassi et al. (2021) compared pixel-based (PB) and object-based (OB) approaches using Random Forest classification algorithm on Landsat 8 data in Google Earth Engine (GEE) for land cover classification in Maiella National Park, central Italy. By integrating Gray-Level Co-occurrence Matrix (GLCM) texture information and utilizing the L8 panchromatic band to enhance segmentation steps, a 15-meter resolution Land Use/Land Cover (LULC) map was produced, showing better performance with the object-based approach when incorporating the panchromatic band (Tassi et al., 2021). The advantage of using Random Forest is its non-parametric nature, ability to handle high dimensionality, and avoidance of overfitting (Tassi et al., 2021). However, its limitation lies in being a black box model with unclear decision tree splitting rules. GEE platform offers the capability to process and analyze vast geospatial information, but its limitations might include the complexity of processing and analysis, requiring a high level of technical knowledge and experience.\n\n\n\n\n\n\n‘Flowchart of the methodology’ (Tassi et al., 2021)"
  },
  {
    "objectID": "week_7.html#reflection",
    "href": "week_7.html#reflection",
    "title": "7  Week 7 - Classification I",
    "section": "7.3 7.3 Reflection",
    "text": "7.3 7.3 Reflection\nThrough this week’s lecture and practical I reviewed the two broad categories of supervised and unsupervised classification, and also learnt about CART, which is a classification method that I had never come across before. Actually, Random Forest Classification was used by myself in my undergraduate dissertation, so I have some familiarity with it. However, I learnt this method through independent study at that time, and my understanding of this method has been further deepened in this module after the teacher’s explanation and review in class.\nI have to be impressed once again with the power of GEE’s functionality. The use of the GEE platform offers the benefit of powerful remote sensing and geospatial analysis capabilities, efficiently facilitating large-scale assessments of land cover and change detection studies. For example, in this week’s practical, using CART and Random Forest to classify the land cover in GEE and reading the literature to classify the use of land cover using different classification methods on the GEE platform, and so on, all of which made me feel the strong computing power of GEE."
  },
  {
    "objectID": "week_7.html#reference-list",
    "href": "week_7.html#reference-list",
    "title": "7  Week 7 - Classification I",
    "section": "7.4 7.4 Reference list",
    "text": "7.4 7.4 Reference list\nNoi Phan, T., Kuch, V., and Lehnert, L. (2020) Land cover classification using google earth engine and random forest classifier-the role of image composition. Remote sensing (Basel, Switzerland). [Online] 12 (15), 2411-. DOI: 10.3390/RS12152411.\nPande, C. B. et al. (2024) Impact of land use/land cover changes on evapotranspiration and model accuracy using Google Earth engine and classification and regression tree modeling. Geomatics, natural hazards and risk. [Online] 15 (1), . DOI: 10.1080/19475705.2023.2290350.\nTassi, A., Gigante, D., Modica, G., Di Martino, L., and Vizzari, M. (2021) Pixel-vs. Object-based landsat 8 data classification in google earth engine using random forest: The case study of maiella national park. Remote sensing (Basel, Switzerland). [Online] 13 (12), 2299-. DOI: 10.3390/rs13122299."
  },
  {
    "objectID": "week_8.html#summary",
    "href": "week_8.html#summary",
    "title": "8  Week 8 - Classification II",
    "section": "8.1 8.1 Summary",
    "text": "8.1 8.1 Summary\nI have taken what I have learnt this week and reviewed the course material and organised it into the following mind map for convenience of review and understanding:\n\n\n\n\n\nMind map of week 8 summary\nThe following 8.1.1 and 8.1.2 summarise the content of all the sub-elements listed in the mind map:\n\n8.1.1 8.1.1 Land Cover Classification (Continued)\nThe need for landcover data (current or historical):\n\nHistorical and current land cover data are crucial for understanding changes on the Earth’s surface, urban expansion, and managing natural resources effectively.\n\nThe use of pre-classified data:\n\nUtilization of pre-classified data sources such as GlobeLand30, the European Space Agency’s Climate Change Initiative (CCI) annual global land cover data, and Dynamic World’s near real-time 10m resolution data is highlighted.\n\nThe Dynamic World project:\n\nIt shows the application of AI methods for processing Sentinel-2 satellite images to estimate tree cover, urban density, or snow coverage for each pixel, encompassing data handling, preprocessing, normalization, and the classification process with Fully Convolutional Neural Networks (FCNN) and Convolutional Neural Networks (CNN).\nAccuracy Assessment:\n\nConfusion matrices are used to assess classification accuracy, though their applicability may be limited for real-time updates or different product comparisons.\nDynamic World data could be compared by other LULC datasets globally and regionally, like ESA CCI, CGLS ProbaV, ESA Sentinel-2 GLC, MapBiomas, and USGS NLCD, to validate accuracy at specific locations in Brazil, Norway, and the United States, involving resolution and land cover class distribution comparisons.\n\n\nObject-Based Image Analysis and Sub-Pixel Analysis:\n\nObject-Based Image Analysis (OBIA) uses the SLIC algorithm to cluster pixels into superpixels based on homogeneity or heterogeneity.\nSub-pixel analysis techniques, including sub-pixel classification, Spectral Mixture Analysis (SMA), and linear spectral unmixing, aim to quantify the proportions of different land cover types within a single pixel.\n\n\n\n8.1.2 8.1.2 Accuracy Accessment\nAccuracy Assessment in Remote Sensing and Machine Learning\n\nProducer’s accuracy, user’s accuracy, and overall accuracy serve as primary metrics for classification model performance evaluation.\nThe Kappa coefficient is employed to measure image classification accuracy, despite its controversies.\nROC curves and AUC values are further utilized to evaluate binary classifier performance.\nF1 Score: The F1 score, a harmonic mean of precision and recall, is crucial, especially in the context of imbalanced datasets. It provides a balance between the precision (user’s accuracy) and recall (producer’s accuracy), serving as a singular metric to assess binary classification systems where false positives and false negatives might have different implications.\n\nChallenges in Accuracy Assessment\n\nThe procurement of suitable test data for accuracy evaluation involves strategies like using new datasets, implementing train/test splits, and applying cross-validation techniques.\nSpatial cross-validation methods for spatial autocorrelation issues between training and testing data sets is used to mitigate the impact of geographical proximity on model evaluation, ensuring that the training and testing data are sufficiently separate to provide an accurate assessment of model performance.\n\n\n\n8.1.3 8.1.3 Practical\nSummary of this week’s practical:\n\nVector Data Processing:\n\nLoaded Level 2 Global Administrative Unit Layers (GAUL) and styled them on the map. By setting the color, width, and fill color (including transparency), the practice demonstrated how to visually enhance vector data representation. Additionally, learned how to filter the dataset based on specific conditions (e.g., administrative unit names) to focus on a particular study area (e.g., Dar es Salaam).\n\nEarth Observation (EO) Data Processing:\n\nSwitched to using Landsat data, mastering skills in setting scale functions, loading imagery, filtering based on dates and study areas, cloud coverage filtering, applying scale functions, extracting medians, and mapping the output. Furthermore, through creating and utilizing functions, learned how to identify and mask clouds or cloud shadows using Quality Assessment (QA) bands, though it was noted that some masked areas were actually sand or suspended matter, not clouds.\n\nImage Classification:\n\nTwo main methods of image classification - sub pixel analysis and object based image analysis (OBIA). Sub pixel analysis involved defining land cover endmembers (e.g., urban, vegetation, and water) and calculating the fraction of each pixel’s endmembers. OBIA involved clustering similar pixels into objects using techniques like K-means and SNIC, followed by analyzing and classifying these objects.\n\nAccuracy Assessment:\n\nExplored methods for assessing the accuracy of sub pixel analysis, including hardening the sub pixel image or comparing it with high resolution imagery to evaluate the precision of the classifications.\n\n\nThese images below show the study area and the final results of the sub pixel analysis and object based image analysis (OBIA) of the remotely sensed data of the area:\nThe study area is Dar es Salaam, and below is a clipped Landsat image of this area (to make it easier to compare the effects of the classification results later on):\n\nStudy area:\n\n\n\n\n\n\nClipped Landsat image of Dar es Salaam\nSub pixel analysis:\nBefore I started this classification I checked the resolution of the images and found that the resolution was not very high, so some features were not easily distinguishable (I also sampled some of the polygon samples using the satellite base map from Google Maps as reference). I thought the result of the classification would be poor, but I didn’t realise that the actual result would look OK to the naked eye compared to the satellite data:\n\n\n\n\n\nThe result of Sub pixel analysis\nObject based image analysis:\nThe effects of the result from this analysis are significantly worse compared to the previous method:\n\n\n\n\n\nThe result of Object based image analysis\nExplanation for the final results:\n\nFrom my observation of Landsat satellite imagery and Google Maps satellite image base map of the study area, I found that the land cover of the area is complex. The mixing of urban and forested or bare or grassy land on the surface of the area is very common. And there is a clear difference between the principle of Object-based classification method and Sub pixel analysis method:\n\nObject based image analysis often performs poorly in areas with high spatial heterogeneity or complex land cover because it relies on spatial proximity and similarity of pixels for classification, potentially missing finer surface variations.\nSub pixel analysis examining the composition within each pixel could more intricately represent mixed land cover situations, making it more effective for capturing small-scale and complex features.\n\nTherefore, the result of Sub pixel analysis is significantly better than the result of Object based image analysis as observed by the naked eyes comparison."
  },
  {
    "objectID": "week_8.html#application",
    "href": "week_8.html#application",
    "title": "8  Week 8 - Classification II",
    "section": "8.2 8.2 Application",
    "text": "8.2 8.2 Application\nSub pixel analysis:\nVos et al. (2019) use an open-source software toolkit named CoastSat, combined with Google Earth Engine (GEE) and Sub-pixel analysis techniques, to extract time series of sandy shoreline positions from publicly available satellite images. It achieves an accuracy of approximately 10 meters (Vos et al., 2019). Sub pixel analysis, applying a robust Sub-pixel resolution shoreline detection algorithm to pre-processed satellite images, which includes supervised image classification and Sub pixel border segmentation, enhances the accuracy and detail recognition capability of shoreline mapping (Vos et al., 2019). However, this method relies on advanced image processing technologies and complex computations, requiring substantial computing resources and algorithm optimization, and may take a longer time when dealing with large-scale data. In addition, despite GEE’s powerful data processing capabilities, users need to have a certain level of familiarity and understanding of the GEE platform and related APIs. When processing extremely large datasets, performance bottlenecks or service limitations might be encountered.\nObject based image analysis:\nAli et al. (2023) using object-based image analysis on the Google Earth Engine platform map the area changes of 480 glaciers on Novaya Zemlya from 1986-1989 to 2019-2021, which reveals a total glacier area reduction of 5.8%. This method has streamlined the process of generating accurate glacier outlines, enhancing efficiency (Ali et al., 2023). The object based image analysis method integrating multispectral Landsat imagery and the computational resources of Google Earth Engine automates the mapping of glacier changes and generates highly accurate glacier outlines with an overall accuracy rate of 96-97% (Ali et al., 2023). While this method effectively reduces the need for manual corrections, it may not perform well in mapping debris-covered glaciers or areas covered with fresh snow and thin cloud layers (Ali et al., 2023). Additionally, reliance on Google Earth Engine might limit access to specific datasets and advanced analysis tools."
  },
  {
    "objectID": "week_8.html#reflection",
    "href": "week_8.html#reflection",
    "title": "8  Week 8 - Classification II",
    "section": "8.3 8.3 Reflection",
    "text": "8.3 8.3 Reflection\nDuring this week I have gained knowledge of unfamiliar or new skills such as Dynamic World Project, Sub pixel analysis, Object based image analysis, all of which are cool and greatly enriched my knowledge of remotely sensed classification methods and accuracy assessment.\nAs it may be largely unfamiliar content, I was not very clear about the structure of the course content, so after reviewing the study material I put together a mind map to make it easier to quickly clarify the structure of content in future reviews. I had never come across the two classification methods sub pixel analysis and Object based image analysis before, but I was impressed with sub pixel analysis during this week’s study. I didn’t realise that sub pixel analysis could classify remote sensing images with low resolution with good results. In the future, this method could be considered for analysing the land cover of images with low resolution and complex land cover."
  },
  {
    "objectID": "week_8.html#reference-list",
    "href": "week_8.html#reference-list",
    "title": "8  Week 8 - Classification II",
    "section": "8.4 8.4 Reference list",
    "text": "8.4 8.4 Reference list\nAli, A., Dunlop, P., Coleman, S., Kerr, D., McNabb, R. W., and Noormets, R. (2023) Glacier area changes in Novaya Zemlya from 1986–89 to 2019–21 using object-based image analysis in Google Earth Engine. Journal of glaciology. [Online] 69 (277), 1305–1316. DOI: 10.1017/jog.2023.18.\nVos, K., Splinter, K. D., Harley, M. D., Simmons, J. A., and Turner, I. L. (2019) CoastSat: A Google Earth Engine-enabled Python toolkit to extract shorelines from publicly available satellite imagery. Environmental modelling & software : with environment data news. [Online] 122104528-. DOI: 10.1016/j.envsoft.2019.104528."
  },
  {
    "objectID": "week_9.html",
    "href": "week_9.html",
    "title": "9  Week 9 - SAR",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  }
]