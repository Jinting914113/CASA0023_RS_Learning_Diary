[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Diary - CASA0023",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  week_2",
    "section": "",
    "text": "For my Xaringan presentation please click or copy the following link:\nhttps://jinting914113.github.io/CASA23_RS/\nNo entries for this week"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3",
    "section": "",
    "text": "4 Week 3 - Remote sensing data:"
  },
  {
    "objectID": "week_3.html#geometric-correction",
    "href": "week_3.html#geometric-correction",
    "title": "3  Week 3",
    "section": "6.1 3.1.1 Geometric Correction",
    "text": "6.1 3.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\n\n6.1.1 How:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\n\n\n6.1.2 Corrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)"
  },
  {
    "objectID": "week_3.html#atmospheric-correction",
    "href": "week_3.html#atmospheric-correction",
    "title": "3  Week 3",
    "section": "6.2 3.1.2 Atmospheric Correction",
    "text": "6.2 3.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\n\n6.2.1 Why is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\n\n\n6.2.2 Corrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance."
  },
  {
    "objectID": "week_3.html#orthorectification-topographic-correction",
    "href": "week_3.html#orthorectification-topographic-correction",
    "title": "3  Week 3",
    "section": "6.3 3.1.3 Orthorectification / Topographic Correction",
    "text": "6.3 3.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\n\n6.3.1 How:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\n\n\n6.3.2 Corrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed."
  },
  {
    "objectID": "week_3.html#radiometric-calibration",
    "href": "week_3.html#radiometric-calibration",
    "title": "3  Week 3",
    "section": "6.4 3.1.4 Radiometric Calibration",
    "text": "6.4 3.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\n\n6.4.1 How:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\n\n\n6.4.2 Corrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\n\n\n6.4.3 Summary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials."
  },
  {
    "objectID": "week_3.html#feathering",
    "href": "week_3.html#feathering",
    "title": "3  Week 3",
    "section": "7.1 3.2.1 Feathering",
    "text": "7.1 3.2.1 Feathering\n\n7.1.1 Data Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\n\n\n7.1.2 Feathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\n\n\n7.1.3 Practical\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area."
  },
  {
    "objectID": "week_3.html#image-enhancement",
    "href": "week_3.html#image-enhancement",
    "title": "3  Week 3",
    "section": "7.2 3.2.2 Image enhancement",
    "text": "7.2 3.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\n\n7.2.0.1 3.2.2.1 Contrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\n\n\n7.2.1 3.2.2.2 Ratio:\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\n\n7.2.1.1 Usage:\nFor instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\n\n\n7.2.1.2 Practical:\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\n\n7.2.2 3.2.2.3 Filtering:\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\n\n7.2.2.1 Usage:\nLow-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\n\n\n7.2.2.2 Practical:\nThe Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\n\n\n7.2.3 3.2.2.4 PCA:\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\n\n7.2.3.1 Usage:\nApplied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\n\n\n7.2.3.2 Practical:\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\n\n7.2.4 3.2.2.5 Texture:\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\n\n7.2.4.1 Usage:\nEnhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\n\n\n7.2.4.2 Practical:\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the GLCMTextures package can be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\n\n7.2.5 3.2.2.6 Fusion:\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\n\n7.2.5.1 Usage:\nIn Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\n\n\n7.2.5.2 Practical:\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data. In R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#geometric-correction-1",
    "href": "week_3.html#geometric-correction-1",
    "title": "3  Week 3",
    "section": "8.1 3.1.1 Geometric Correction",
    "text": "8.1 3.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\n\n8.1.1 How:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\n\n\n8.1.2 Corrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)"
  },
  {
    "objectID": "week_3.html#atmospheric-correction-1",
    "href": "week_3.html#atmospheric-correction-1",
    "title": "3  Week 3",
    "section": "8.2 3.1.2 Atmospheric Correction",
    "text": "8.2 3.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\n\n8.2.1 Why is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\n\n\n8.2.2 Corrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance."
  },
  {
    "objectID": "week_3.html#orthorectification-topographic-correction-1",
    "href": "week_3.html#orthorectification-topographic-correction-1",
    "title": "3  Week 3",
    "section": "8.3 3.1.3 Orthorectification / Topographic Correction",
    "text": "8.3 3.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\n\n8.3.1 How:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\n\n\n8.3.2 Corrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed."
  },
  {
    "objectID": "week_3.html#radiometric-calibration-1",
    "href": "week_3.html#radiometric-calibration-1",
    "title": "3  Week 3",
    "section": "8.4 3.1.4 Radiometric Calibration",
    "text": "8.4 3.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\n\n8.4.1 How:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\n\n\n8.4.2 Corrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\n\n\n8.4.3 Summary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials."
  },
  {
    "objectID": "week_3.html#feathering-1",
    "href": "week_3.html#feathering-1",
    "title": "3  Week 3",
    "section": "9.1 3.2.1 Feathering",
    "text": "9.1 3.2.1 Feathering\n\n9.1.1 Data Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\n\n\n9.1.2 Feathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\n\n\n9.1.3 Practical\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area."
  },
  {
    "objectID": "week_3.html#image-enhancement-1",
    "href": "week_3.html#image-enhancement-1",
    "title": "3  Week 3",
    "section": "9.2 3.2.2 Image enhancement",
    "text": "9.2 3.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\n\n9.2.0.1 3.2.2.1 Contrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\n\n\n9.2.1 3.2.2.2 Ratio:\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\n\n9.2.1.1 Usage:\nFor instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\n\n\n9.2.1.2 Practical:\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\n\n9.2.2 3.2.2.3 Filtering:\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\n\n9.2.2.1 Usage:\nLow-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\n\n\n9.2.2.2 Practical:\nThe Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\n\n\n9.2.3 3.2.2.4 PCA:\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\n\n9.2.3.1 Usage:\nApplied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\n\n\n9.2.3.2 Practical:\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\n\n9.2.4 3.2.2.5 Texture:\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\n\n9.2.4.1 Usage:\nEnhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\n\n\n9.2.4.2 Practical:\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the GLCMTextures package can be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\n\n9.2.5 3.2.2.6 Fusion:\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\n\n9.2.5.1 Usage:\nIn Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\n\n\n9.2.5.2 Practical:\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data. In R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#content-summary",
    "href": "week_3.html#content-summary",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.1 3.1 Content summary:",
    "text": "3.1 3.1 Content summary:\nThis is an outline of what I have learnt this week and I will follow these outlines to summarise the knowledge or skills I have learnt:\n\n\n\n\n\nMind map for the summary outline\n\n3.1.1 3.1.1 Corrections\nBecause of the atmosphere, unevenness of the earth’s surface and other factors, remote sensing images are sometimes inaccurate, so we need to correct them. For example, as the scan line corrector on Landsat 7 failed, we need to correct the remotely sensed images.\nRemote sensed products are now often “corrected” to “Analysis Ready Data” (ARD), for instance, Landsat ARD products are processed through LEDPAS and L8SR algorithms to achieve surface reflectance standards, utilizing advanced algorithms and data processing techniques like LaSRC to provide users with processed surface reflectance data. However, when dealing with data not processed as ARD (e.g., very high resolution, drone data), understanding how the data was created remains crucial.\n\n3.1.1.1 3.1.1.1 Geometric Correction\nGeometric correction involves correcting spatial distortions in remote sensed images caused by the sensor’s angle, uneven ground, wind (from plane) and and Rotation of the earth (from satellite).\nHow:\nIt eliminates spatial biases by resampling and transforming images based on ground control points or other reference data to correct their spatial positions.\nCorrected by who:\nRemote sensing data providers or professional users (who need to ensure spatial accuracy of the remotely sensed data)\n\n\n3.1.1.2 3.1.1.2 Atmospheric Correction\nIt deals with the effects of atmospheric scattering and absorption (or Topographic attenuation) in remote sensing images to obtain the true reflectance of the Earth’s surface.\nWhy is it unnecessary in some cases？\nAtmospheric correction is often unnecessary when classifying a single image, independently classifying multi-date imagery, creating composite images, or using training data extracted from all data, as the precise correction of atmospheric factors has limited impact on the final outcomes in these scenarios.\nHow:\n\nAtmospheric correction could be achieved through relative methods, such as normalizing the intensities of different bands within a single image, normalizing intensities of bands from many dates to one, dark object subtraction (DOS), or histogram adjustment to approximate the elimination of atmospheric effects. Absolute methods convert digital brightness values into scaled surface reflectance using atmospheric radiative transfer models, but it requires atmospheric models, local atmospheric visibility data, and tools like ACORN, FLAASH, etc.\nIn the practical of this week, we use Dark Object Subtraction (DOS). It is a simple and effective method for atmospheric correction that reduces atmospheric effects by identifying the atmospheric scatter value represented by the darkest pixel in an image and subtracting it from the entire image.\n\nCorrected by who:\nPrimarily carried out by data providers or specialized researchers using dedicated software, aimed at eliminating atmospheric effects to obtain true surface reflectance.\n\n\n3.1.1.3 3.1.1.3 Orthorectification / Topographic Correction\nOrthorectification / topographic correction involves correcting deformations caused by terrain in images using terrain information, giving them true map geometric characteristics. It requires sensor geometry and an elevation model.\nHow:\nOrthorectification / topographic correction involves removing terrain-induced distortions in images by considering sensor geometry and utilizing a Digital Elevation Model (DEM), ensuring each pixel is depicted as if captured from directly overhead for a clear, terrain-unaffected image. This process typically involves using specialized software (e.g., QGIS, SAGA GIS, or R packages like topocorr and RStoolbox) and formulas (e.g., cosine correction, Minnaert correction, etc.) to achieve this.\nCorrected by who:\nIt is typically done by data providers as a preprocessing step or customized by researchers using GIS software as needed.\n\n\n3.1.1.4 3.1.1.4 Radiometric Calibration\nRadiometric correction is the process of adjusting remote sensing data to eliminate effects caused by sensor characteristics and atmospheric conditions, making the data reflect true surface radiometric properties.\nHow:\nRadiometric correction is achieved by applying calibration parameters (such as gain and bias) to each pixel value of the digital image, converting digital number (DN) to spectral radiance.\nCorrected by who:\nBy remote sensing data providers for basic processing or by end-users for more in-depth corrections to meet specific application requirements.\nSummary of the jargon:\n\nDigital Number (DN): It is an uncalibrated value representing the intensity of electromagnetic radiation of a pixel, without any unit.\nSpectral Radiance: It is the amount of light within a band from a sensor in the field of view per unit area, solid angle, and wavelength, measured in W/m²/sr/μm.\nSensor Calibration (Gain and Bias): It defines the relationship between digital number and spectral radiance through gain and bias parameters.\nTop of Atmosphere (TOA) Radiance: It refers to the amount of light in meaningful units observed by the sensor, including effects of light source, atmosphere, and surface material.\nReflectance: It is the ratio of the amount of light reflected by a target to the amount of light it receives, an inherent property of the material.\nTop of Atmosphere (TOA) Reflectance: It is the radiance adjusted to remove effects of the light source, reflecting the inherent properties of surface materials.\nSurface Reflectance: It is the reflectance with light source and atmospheric effects removed, more accurately representing the reflective characteristics of surface materials.\n\n\n\n\n3.1.2 3.1.2 Data joining and enhancement\n\n3.1.2.1 3.1.2.1 Feathering\nData Joining:\nIt refers to the process in remote sensing of merging multiple datasets (such as images) into one continuous large image or mosaic, commonly known as mosaicking. ‘Mosaicking in with a standard method isn’t appropriate for satellite imagery’.\nFeathering:\n\nIt is an image processing technique used to smooth the transition area between images in remote sensing data joining, eliminating seam lines to create a seamless image mosaic.\nMethod of Feathering: It involves sampling representative samples within the overlap area, adjusting image brightness values using a histogram matching algorithm to achieve a smooth brightness transition between the two images.\n\nPractical:\nWhen a single image does not cover the entire study area, downloading two or more satellite image tiles and mosaicking (or merging) them together, using functions like ‘mosaic’ in the ‘terra’ library for an average merge, could extend the coverage of the study area.\n\n\n3.1.2.2 3.1.2.2 Image enhancement\nImage enhancement is the process of improving the visual appearance or results of an image by increasing the contrast and distinguishability between features in the image.\nContrast Enhancement:\n\nEnhancing the contrast of an image by expanding its dynamic range, making the light and dark areas more pronounced.\nUsage: Achieved through methods like minimum-maximum, percentage linear and standard deviation, piecewise linear contrast stretch.\n\nRatio:\n\nRatio image enhancement is a technique that emphasizes or reveals specific landscape features by calculating the ratio of values between two spectral bands.\nUsage: For instance, the Normalized Burn Ratio (NBR) highlights burned areas or vegetation health by calculating the ratio of the difference to the sum of the Near-Infrared (NIR) and Short-Wave Infrared (SWIR) bands.\nPractical:\n\nRatio is a method that emphasizes or exaggerates certain landscape features based on the characteristic that healthy vegetation has higher reflectance in the NIR and absorbs more in the Red wavelength. The Normalized Difference Vegetation Index (NDVI) uses this trait to highlight areas of healthy vegetation.\nIn R, NDVI can be calculated using a specific formula, and the data can then be reclassified to highlight areas where NDVI is equal to or greater than 0.2.\n\n\nFiltering:\n\nImproving image quality by applying low-pass (smoothing local variations) or high-pass (enhancing local details) filters to the image.\nUsage: Low-pass filtering smooths the image by summing all pixels in a 3x3 window and dividing by 9; high-pass filtering is used to highlight edges and texture in the image.\nPractical: The Laplacian filter can be used to enhance edges and details in an image. In R, a 3x3 window filter could be applied to a specific band using the focal function from the terra package.\n\nPCA:\n\nA technique to transform multispectral data into an uncorrelated smaller dataset, retaining most of the original information and reducing future computational load.\nUsage: Applied by using functions like ‘prcomp()’ in the ‘terra’ package, extracting principal components based on the principle of maximizing variance within the dataset.\nPractical:\n\nPrincipal Component Analysis (PCA) aims to reduce the dimensionality of data. By centering and scaling the data, it allows for the comparison of data measured in different ways (e.g., spectral and texture data). PCA takes advantage of multicollinearity to create new, uncorrelated variables.\nIn R, PCA can be performed using the prcomp function, and the PCA analysis results can be mapped using the predict function.\n\n\nTexture:\n\nIt reveals surface structure and compositional features focusing on the spatial variation of grayscale values in an image.\nUsage: Enhancing image texture by calculating first-order (occurrences) and second-order (relationships between pixel pairs) statistics, and applying co-occurrence matrices to analyze the angular relationships and distances between pixels.\nPractical:\n\nTexture analysis aims to identify the relationships between pixels in an image, often wanting to see how pixel-to-pixel relationships differ in different parts of the image. By calculating the GLCM for small areas and then recording its texture measure to cover the entire image, the variation in pixel relationships across different locations can be quantified.\nIn R, the ‘GLCMTextures’ package is able to be used to compute texture, and the original data may need to be converted back to a float for use.\n\n\nFusion:\n\nCombining data from multiple sensors/sources to enhance the resolution and quality of an image.\nUsage: In Landsat data, multispectral data (30m) can be sharpened by fusion with the panchromatic band (15m), typically applied to visible bands.\nPractical:\n\nData fusion is the process of appending new raster data to existing data or making a new raster dataset with different bands, which could be done by combining the texture measure (and the original spectral data) with the existing data.\nIn R, the texture data and original data can be combined using the ‘c’ function from the ‘terra’ package."
  },
  {
    "objectID": "week_3.html#applications-of-the-content",
    "href": "week_3.html#applications-of-the-content",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.2 3.2 Applications of the content:",
    "text": "3.2 3.2 Applications of the content:\nAs I mentioned in my summary, although many remote sensing image products have now been corrected by data providers, there are also images that are uncorrected or require more demanding corrections. According to Aasen et al. (2018), the article discusses sensor technology, measurement procedures, and data correction workflows (workflows graph as shown below (‘Empirical Line Correction’, which is covered in our lecture)) in UAV spectral remote sensing, including geometric processing, radiometric calibration, scene reflectance generation, and scene reflectance correction. These correction techniques help in accurately representing the energy reflected from the environment as pixels in a data product. The article provides a comprehensive evaluation of the state-of-the-art methods in UAV spectral remote sensing, incorporating over a decade of experimentation and operational demonstrations to offer clear guidance for acquiring reliable data. Despite providing detailed techniques and correction workflows, implementing these complex techniques and processes may be challenging for beginners and small project teams.\n\n\n\n\n\nData correction workflows (Aasen et al., 2018)\nRatio enhancement is widely used in reality and NDVI is a very popular application of ratio enhancement, so the application of NDVI is crucial. NDVI (Normalized Difference Vegetation Index) is a numerical indicator that uses the visible and near-infrared bands of the electromagnetic spectrum to assess vegetation health and density. It is calculated using the following formula:\n\n\n\n\n\nThere are several papers mentioning the application of NDVI, but certainly the application of NDVI has some disadvantages besides the advantages. Sims et al. (2014) uses NDVI as the main tool to assess vegetation responses to drought, finding that forest ecosystems maintain higher greenness compared to non-forest ecosystems under drought conditions, with little change in greenness even under extreme drought. In this essay, NDVI assists in improving predictions of water stress impacts on forest ecosystems at low spectral sensitivity levels by demonstrating differences in drought sensitivity among ecosystems (Sims et al., 2014). For example, predicting declines in carbon uptake in forest ecosystems by observing NDVI declines in adjacent ecosystems with high spectral sensitivity (Sims et al., 2014). NDVI could reveal differential responses of forest and non-forest ecosystems to drought, providing an effective tool for predicting and assessing carbon uptake in forest ecosystems under global climate change (Sims et al., 2014). However, NDVI may saturate in dense forest systems, limiting its ability to accurately reflect physiological responses of forest ecosystems under extreme drought conditions.\nIn addition, NDVI is utilized as a tool to monitor the performance and changes of certain strategic crops in Egypt throughout the growing season, showcasing a time series analysis of crop growth stages and vegetation health (Farg et al., 2019).\n\n\n\n\n\nThe application of NDVI on the crop (‘Different NDVI for June, July and September for the study area.’) (Farg et al., 2019)\nNDVI shows the capability for accurate crop classification under complex terrain conditions, enhancing the precision of crop discrimination through analysis of remote sensing data over different periods (Farg et al., 2019). Although time-series analysis based on NDVI is instrumental in understanding crop growth and health, it may not be sufficiently sensitive to the minute spectral variations among different crop types. This limitation is particularly pronounced in the initial phases of crop development, which could affect the precise identification and categorization of certain crop types."
  },
  {
    "objectID": "week_3.html#personal-reflection",
    "href": "week_3.html#personal-reflection",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.3 3.3 Personal reflection:",
    "text": "3.3 3.3 Personal reflection:\nThis week’s summary section is organised in a more textual way. Although I have studied remote sensing in my undergraduate studies and have worked with some of the applications of remotely sensing, we have only had a superficial understanding of image correction because many remote sensing imagery products have been corrected in their own right. In this week’s work, we covered a lot of knowledge and skills of image corrections and image enhancement, and I would like to make up for my shortcomings, so I’ve put together more of a basics section.\nAlthough the principles of the correction technique for remotely sensed images are difficult, the technique could be applied to high-resolution images taken by UAVs and is constantly being developed, which is probably one of the reasons why the technique is so attractive. Data correction is an early stage in the processing and analysis of remotely sensed data (and is needed anyway, even though much of the data that may be available to the user may have already been corrected by the data provider) and is very important and meaningful, as it is the foundation of all analyses related to remotely sensed data. If the individual works in a company related to remotely sensed image processing, then the correction of remotely sensed images may be a necessary element to master.\nBefore studying this course, image enhancement was something I was exposed to a lot, especially ratio enhancement. Image enhancement makes it easier to recognise and extract features of water bodies, plants and other features. This part is also very widely used in reality. In China, the Bureau of Natural Resources often uses image enhancement to survey natural resources such as forests and so on.\nOverall, I have learnt knowledge and skills in remotely sensed image product corrections and image joining and enhancement this week.This allows me to supplement the shortcomings of my undergraduate learning content to some extent, and enriches my knowledge, which may allow me to be more competitive in my future job."
  },
  {
    "objectID": "week_3.html#week-3---remote-sensing-data",
    "href": "week_3.html#week-3---remote-sensing-data",
    "title": "3  Week 3",
    "section": "3.1 Week 3 - Remote sensing data:",
    "text": "3.1 Week 3 - Remote sensing data:"
  },
  {
    "objectID": "week_3.html#reference-list",
    "href": "week_3.html#reference-list",
    "title": "3  Week 3 - Remotely Sensed Data Corrections and Data joining and Enhancement",
    "section": "3.4 3.4 Reference list",
    "text": "3.4 3.4 Reference list\nAasen, H., Honkavaara, E., Lucieer, A., & Zarco-Tejada, P. J. (2018) Quantitative Remote Sensing at Ultra-High Resolution with UAV Spectroscopy: A Review of Sensor Technology, Measurement Procedures, and Data Correction Workflows. Remote sensing (Basel, Switzerland). [Online] 10 (7), 1091-. https://doi.org/10.3390/rs10071091\nFarg, E., Ramadan, M. N., and Arafat, S. M. (2019) Classification of some strategic crops in Egypt using multi remotely sensing sensors and time series analysis. The Egyptian journal of remote sensing and space sciences. [Online] 22 (3), 263–270. https://doi.org/10.1016/j.ejrs.2019.07.002\nSims, D. A., Brzostek, E. R., Rahman, A. F., Dragoni, D., and Phillips, R. P. (2014) An improved approach for remotely sensing water stress impacts on forest C uptake. Global change biology. [Online] 20 (9), 2856–2866. https://doi.org/10.1111/gcb.12537"
  }
]